{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejasajja/Suggest_CV/blob/main/Resume_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WP5HbfiqnVzM"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.1.4\n",
        "!pip install langchain-google-genai==0.0.6\n",
        "!pip install langchain-openai==0.0.2.post1\n",
        "!pip install langchain-community==0.0.19\n",
        "!pip install cohere\n",
        "!pip install Pillow\n",
        "!pip install faiss-cpu\n",
        "!pip install python-dotenv\n",
        "!pip install markdown\n",
        "!pip install pdfminer.six\n",
        "!pip install streamlit==1.28.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "y55wrP0muAt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0__xEIbp59r"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A34v4enlncvu"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from langchain_community.document_loaders import PDFMinerLoader, TextLoader, Docx2txtLoader\n",
        "\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CohereRerank\n",
        "from langchain_community.llms import Cohere\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Other libraries\n",
        "import datetime, json, os, tiktoken\n",
        "from IPython.display import Markdown\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from PIL import Image\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VMqawM6ynen1"
      },
      "outputs": [],
      "source": [
        "ASSISTAN_LANGUAGE = \"english\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O4Jbwr6unhUI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "openai_api_key =\n",
        "google_api_key =\n",
        "cohere_api_key =\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Q_KTsg3Govc_"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PDFMinerLoader\n",
        "\n",
        "def langchain_document_loader(file_path):\n",
        "    \"\"\"Load and split a PDF file in Langchain.\n",
        "    Parameters:\n",
        "        - file_path (str): path of the file.\n",
        "    Output:\n",
        "        - documents: list of Langchain Documents.\"\"\"\n",
        "\n",
        "    if file_path.endswith(\".pdf\"):\n",
        "        loader = PDFMinerLoader(file_path=file_path)\n",
        "    else:\n",
        "        print(\"You can only upload .pdf files!\")\n",
        "\n",
        "    # 1. Load and split documents\n",
        "    documents = loader.load_and_split()\n",
        "\n",
        "    # 2. Update the metadata: add document number to metadata\n",
        "    for i in range(len(documents)):\n",
        "        documents[i].metadata = {\n",
        "            \"source\": documents[i].metadata[\"source\"],\n",
        "            \"doc_number\": i,\n",
        "        }\n",
        "\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gWlOasWowjT"
      },
      "outputs": [],
      "source": [
        "documents = langchain_document_loader(\"/content/tsajja_resume.pdf\")\n",
        "print(\"number of documents:\",len(documents))\n",
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVKKGb88o3mT",
        "outputId": "ba15bd96-036a-4a32-b461-5868298aaa65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens per document: [836, 340]\n",
            "Sum of tokens (documents): 1176\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "def tiktoken_tokens(documents,model=\"gpt-3.5-turbo-0125\"):\n",
        "    \"\"\"Use tiktoken (tokeniser for OpenAI models) to return a list of token length per document.\"\"\"\n",
        "\n",
        "    encoding = tiktoken.encoding_for_model(model) # returns the encoding used by the model.\n",
        "    tokens_length = [len(encoding.encode(doc)) for doc in documents]\n",
        "\n",
        "    return tokens_length\n",
        "\n",
        "\n",
        "# Calculate the number of tokens in each document.\n",
        "documents_length = tiktoken_tokens([doc.page_content for doc in documents],model=\"gpt-3.5-turbo-0125\")\n",
        "\n",
        "print(\"Number of tokens per document:\",documents_length)\n",
        "print(\"Sum of tokens (documents):\",sum(documents_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "J5FJXY3Xo9h3"
      },
      "outputs": [],
      "source": [
        "def select_embeddings_model(LLM_service=\"OpenAI\"):\n",
        "    \"\"\"Connect to the embeddings API endpoint by specifying the name of the embedding model.\"\"\"\n",
        "    if LLM_service == \"OpenAI\":\n",
        "        embeddings = OpenAIEmbeddings(\n",
        "            model='text-embedding-ada-002',\n",
        "            api_key=openai_api_key)\n",
        "\n",
        "    if LLM_service == \"Google\":\n",
        "        embeddings = GoogleGenerativeAIEmbeddings(\n",
        "            model=\"models/embedding-001\",\n",
        "            google_api_key=google_api_key\n",
        "        )\n",
        "    return embeddings\n",
        "\n",
        "embeddings_OpenAI = select_embeddings_model(LLM_service=\"OpenAI\")\n",
        "embeddings_google = select_embeddings_model(LLM_service=\"Google\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "def create_vectorstore(embeddings, documents):\n",
        "    \"\"\"Create a Faiss vector database.\"\"\"\n",
        "    vector_store = FAISS.from_documents(documents=documents, embedding=embeddings)\n",
        "\n",
        "    return vector_store"
      ],
      "metadata": {
        "id": "2DaHs2ZlujLl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "vector_store_OpenAI = create_vectorstore(embeddings=embeddings_OpenAI,documents=documents)\n",
        "vector_store_google = create_vectorstore(embeddings=embeddings_google,documents=documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N81t27MNulGA",
        "outputId": "2630e6c4-95d8-43b7-a4bd-5a618da7aa82"
      },
      "execution_count": 17,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 108 ms, sys: 16.4 ms, total: 124 ms\n",
            "Wall time: 1.56 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Vectorstore_backed_retriever(vectorstore,search_type=\"similarity\",k=4,score_threshold=None):\n",
        "    \"\"\"create a vectorsore-backed retriever\n",
        "    Parameters:\n",
        "        search_type: Defines the type of search that the Retriever should perform.\n",
        "            Can be \"similarity\" (default), \"mmr\", or \"similarity_score_threshold\"\n",
        "        k: number of documents to return (Default: 4)\n",
        "        score_threshold: Minimum relevance threshold for similarity_score_threshold (default=None)\n",
        "    \"\"\"\n",
        "    search_kwargs={}\n",
        "    if k is not None:\n",
        "        search_kwargs['k'] = k\n",
        "    if score_threshold is not None:\n",
        "        search_kwargs['score_threshold'] = score_threshold\n",
        "\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type=search_type,\n",
        "        search_kwargs=search_kwargs\n",
        "    )\n",
        "    return retriever\n",
        "\n",
        "\n",
        "base_retriever_OpenAI = Vectorstore_backed_retriever(vector_store_OpenAI,\"similarity\",k=min(4,len(documents)))\n",
        "base_retriever_google = Vectorstore_backed_retriever(vector_store_google,\"similarity\",k=min(4,len(documents)))"
      ],
      "metadata": {
        "id": "Bghe3C1Xumke"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CohereRerank\n",
        "from langchain_community.llms import Cohere\n",
        "\n",
        "def CohereRerank_retriever(\n",
        "    base_retriever,\n",
        "    cohere_api_key,cohere_model=\"rerank-multilingual-v2.0\", top_n=2\n",
        "):\n",
        "    \"\"\"Build a ContextualCompressionRetriever using Cohere Rerank endpoint to reorder the results based on relevance.\n",
        "    Parameters:\n",
        "       base_retriever: a Vectorstore-backed retriever\n",
        "       cohere_api_key: the Cohere API key\n",
        "       cohere_model: The Cohere model can be either 'rerank-english-v2.0' or 'rerank-multilingual-v2.0', with the latter being the default.\n",
        "       top_n: top n results returned by Cohere rerank, default = 2.\n",
        "    \"\"\"\n",
        "\n",
        "    compressor = CohereRerank(\n",
        "        cohere_api_key=cohere_api_key,\n",
        "        model=cohere_model,\n",
        "        top_n=top_n\n",
        "    )\n",
        "\n",
        "    retriever_Cohere = ContextualCompressionRetriever(\n",
        "        base_compressor=compressor,\n",
        "        base_retriever=base_retriever\n",
        "    )\n",
        "\n",
        "    return retriever_Cohere"
      ],
      "metadata": {
        "id": "FNiSmBHtutTG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_Cohere_google = CohereRerank_retriever(\n",
        "    base_retriever=base_retriever_google,\n",
        "    cohere_api_key=cohere_api_key,\n",
        "    cohere_model=\"rerank-multilingual-v2.0\",\n",
        "    top_n=2\n",
        ")\n",
        "\n",
        "retriever_Cohere_openAI = CohereRerank_retriever(\n",
        "    base_retriever=base_retriever_OpenAI,\n",
        "    cohere_api_key=cohere_api_key,\n",
        "    cohere_model=\"rerank-multilingual-v2.0\",\n",
        "    top_n=2\n",
        ")"
      ],
      "metadata": {
        "id": "dODeuyYluvov"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"Extract the job title and company name of the first work experience.\"\n",
        "\n",
        "# most_relevant_docs = retriever_Cohere_google.get_relevant_documents(query)\n",
        "\n",
        "# for i in range(len(most_relevant_docs)):\n",
        "#     print(f\"\"\"Most similar doc id : {most_relevant_docs[i].metadata['doc_number']} \"\"\")"
      ],
      "metadata": {
        "id": "wHB0dV3Iu2so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def instantiate_LLM(LLM_provider,api_key,temperature=0.5,top_p=0.95,model_name=None):\n",
        "    \"\"\"Instantiate LLM in Langchain.\n",
        "    Parameters:\n",
        "        LLM_provider (str): the LLM provider; in [\"OpenAI\",\"Google\",\"HuggingFace\"]\n",
        "        model_name (str): in [\"gpt-3.5-turbo\", \"gpt-3.5-turbo-0125\", \"gpt-4-turbo-preview\",\n",
        "            \"gemini-pro\", \"mistralai/Mistral-7B-Instruct-v0.2\"].\n",
        "        api_key (str): google_api_key or openai_api_key or huggingfacehub_api_token\n",
        "        temperature (float): Range: 0.0 - 1.0; default = 0.5\n",
        "        top_p (float): : Range: 0.0 - 1.0; default = 1.\n",
        "    \"\"\"\n",
        "    if LLM_provider == \"OpenAI\":\n",
        "        llm = ChatOpenAI(\n",
        "            api_key=api_key,\n",
        "            model=model_name,\n",
        "            temperature=temperature,\n",
        "            model_kwargs={\n",
        "                \"top_p\": top_p\n",
        "            }\n",
        "        )\n",
        "    if LLM_provider == \"Google\":\n",
        "        llm = ChatGoogleGenerativeAI(\n",
        "            google_api_key=api_key,\n",
        "            # model=\"gemini-pro\",\n",
        "            model=model_name,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            convert_system_message_to_human=True\n",
        "        )\n",
        "    return llm"
      ],
      "metadata": {
        "id": "YB97xfMRu4PI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_LLM_and_retriever(provider=\"OpenAI\",model_name=\"gpt-3.5-turbo-0125\",temperature=0.0,top_p=0.95):\n",
        "    if provider==\"OpenAI\":\n",
        "        llm = instantiate_LLM(\n",
        "            \"OpenAI\",\n",
        "            api_key=openai_api_key,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            model_name=model_name\n",
        "        )\n",
        "        retriever = retriever_Cohere_openAI\n",
        "    else: # \"Google\"\n",
        "        llm = instantiate_LLM(\n",
        "            LLM_provider=\"Google\",\n",
        "            api_key=google_api_key,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            model_name=\"gemini-pro\"\n",
        "        )\n",
        "        retriever = retriever_Cohere_google\n",
        "\n",
        "    return llm,retriever"
      ],
      "metadata": {
        "id": "u6md5r5Zu8XR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XtSvlY3TvLbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm,retriever = set_LLM_and_retriever(provider=\"Google\",temperature=0.0)"
      ],
      "metadata": {
        "id": "82tFXZ49vDTx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "templates = {}\n",
        "\n",
        "# 2.1 Contact information Section\n",
        "templates[\n",
        "    \"Contact__information\"\n",
        "] = \"\"\"Extract and evaluate the contact information. \\\n",
        "Output a dictionary with the following keys:\n",
        "- candidate__name\n",
        "- candidate__title\n",
        "- candidate__location\n",
        "- candidate__email\n",
        "- candidate__phone\n",
        "- candidate__social_media: Extract a list of all social media profiles, blogs or websites.\n",
        "- evaluation__ContactInfo: Evaluate in {language} the contact information.\n",
        "- score__ContactInfo: Rate the contact information by giving a score (integer) from 0 to 100.\n",
        "\"\"\"\n",
        "\n",
        "# 2.2. Summary Section\n",
        "templates[\n",
        "    \"CV__summary\"\n",
        "] = \"\"\"Extract the summary and/or objective section. This is a separate section of the resume. \\\n",
        "If the resume doed not contain a summary and/or objective section, then simply write \"unknown\".\"\"\"\n",
        "\n",
        "# 2.3. WORK Experience Section\n",
        "\n",
        "templates[\n",
        "    \"Work__experience\"\n",
        "] = \"\"\"Extract all work experiences. For each work experience:\n",
        "1. Extract the job title.\n",
        "2. Extract the company.\n",
        "3. Extract the start date and output it in the following format: \\\n",
        "YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
        "4. Extract the end date and output it in the following format: \\\n",
        "YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
        "5. Output a dictionary with the following keys: job__title, job__company, job__start_date, job__end_date.\n",
        "\n",
        "Format your response as a list of dictionaries.\n",
        "\"\"\"\n",
        "\n",
        "# 2.4. Projects Section\n",
        "templates[\n",
        "    \"CV__Projects\"\n",
        "] = \"\"\"Include any side projects outside the work experience.\n",
        "For each project:\n",
        "1. Extract the title of the project.\n",
        "2. Extract the start date and output it in the following format: \\\n",
        "YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
        "3. Extract the end date and output it in the following format: \\\n",
        "YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
        "4. Output a dictionary with the following keys: project__title, project__start_date, project__end_date.\n",
        "\n",
        "Format your response as a list of dictionaries.\n",
        "\"\"\"\n",
        "\n",
        "# 2.5. Education Section\n",
        "templates[\n",
        "    \"CV__Education\"\n",
        "] = \"\"\"Extract all educational background and academic achievements.\n",
        "For each education achievement:\n",
        "1. Extract the name of the college or the high school.\n",
        "2. Extract the earned degree. Honors and achievements are included.\n",
        "3. Extract the start date and output it in the following format: \\\n",
        "YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
        "4. Extract the end date and output it in the following format: \\\n",
        "YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
        "5. Output a dictionary with the following keys: edu__college, edu__degree, edu__start_date, edu__end_date.\n",
        "\n",
        "Format your response as a list of dictionaries.\n",
        "\"\"\"\n",
        "\n",
        "templates[\n",
        "    \"Education__evaluation\"\n",
        "] = \"\"\"Your task is to perform the following actions:\n",
        "1. Rate the quality of the Education section by giving an integer score from 0 to 100.\n",
        "2. Evaluate (in three sentences and in {language}) the quality of the Education section.\n",
        "3. Output a dictionary with the following keys: score__edu, evaluation__edu.\n",
        "\"\"\"\n",
        "\n",
        "# 2.6. Skills\n",
        "templates[\n",
        "    \"candidate__skills\"\n",
        "] = \"\"\"Extract the list of soft and hard skills from the skill section. Output a list.\n",
        "The skill section is a separate section.\n",
        "\"\"\"\n",
        "\n",
        "templates[\n",
        "    \"Skills__evaluation\"\n",
        "] = \"\"\"Your task is to perform the following actions:\n",
        "1. Rate the quality of the Skills section by giving an integer score from 0 to 100.\n",
        "2. Evaluate (in three sentences and in {language}) the quality of the Skills section.\n",
        "3. Output a dictionary with the following keys: score__skills, evaluation__skills.\n",
        "\"\"\"\n",
        "\n",
        "# 2.7. Languages\n",
        "templates[\n",
        "    \"CV__Languages\"\n",
        "] = \"\"\"Extract all the languages that the candidate can speak. For each language:\n",
        "1. Extract the language.\n",
        "2. Extract the fluency. If the fluency is not available, then simply write \"unknown\".\n",
        "3. Output a dictionary with the following keys: spoken__language, language__fluency.\n",
        "\n",
        "Format your response as a list of dictionaries.\n",
        "\"\"\"\n",
        "\n",
        "templates[\n",
        "    \"Languages__evaluation\"\n",
        "] = \"\"\" Your task is to perform the following actions:\n",
        "1. Rate the quality of the language section by giving an integer score from 0 to 100.\n",
        "2. Evaluate (in three sentences and in {language}) the quality of the language section.\n",
        "3. Output a dictionary with the following keys: score__language,evaluation__language.\n",
        "\"\"\"\n",
        "\n",
        "# 2.8. Certifications\n",
        "templates[\n",
        "    \"CV__Certifications\"\n",
        "] = \"\"\"Extraction of all certificates other than education background and academic achievements. \\\n",
        "For each certificate:\n",
        "1. Extract the title of the certification.\n",
        "2. Extract the name of the organization or institution that issues the certification.\n",
        "3. Extract the date of certification and output it in the following format: \\\n",
        "YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
        "4. Extract the certification expiry date and output it in the following format: \\\n",
        "YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
        "5. Extract any other information listed about the certification. if not found, then simply write \"unknown\".\n",
        "6. Output a dictionary with the following keys: certif__title, certif__organization, certif__date, certif__expiry_date, certif__details.\n",
        "\n",
        "Format your response as a list of dictionaries.\n",
        "\"\"\"\n",
        "\n",
        "templates[\n",
        "    \"Certif__evaluation\"\n",
        "] = \"\"\"Your task is to perform the following actions:\n",
        "1. Rate the certifications by giving an integer score from 0 to 100.\n",
        "2. Evaluate (in three sentences and in {language}) the certifications and the quality of the text.\n",
        "3. Format your response as a dictionary with the following keys: score__certif,evaluation__certif.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# 3. PROMPTS\n",
        "\n",
        "PROMPT_IMPROVE_SUMMARY = \"\"\"Your are given a resume (delimited by <resume></resume>) \\\n",
        "and a summary (delimited by <summary></summary>).\n",
        "1. In {language}, evaluate the summary (format and content) .\n",
        "2. Rate the summary by giving an integer score from 0 to 100. \\\n",
        "If the summary is \"unknown\", the score is 0.\n",
        "3. In {language}, strengthen the summary. The summary should not exceed 5 sentences. \\\n",
        "If the summary is \"unknown\", generate a strong summary in {language} with no more than 5 sentences. \\\n",
        "Please include: years of experience, top skills and experiences, some of the biggest achievements, and finally an attractive objective.\n",
        "4. Format your response as a dictionary with the following keys: evaluation__summary, score__summary, CV__summary_enhanced.\n",
        "\n",
        "<summary>\n",
        "{summary}\n",
        "</summary>\n",
        "------\n",
        "<resume>\n",
        "{resume}\n",
        "</resume>\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_IMPROVE_WORK_EXPERIENCE = \"\"\"you are given a work experience text delimited by triple backticks.\n",
        "1. Rate the quality of the work experience text by giving an integer score from 0 to 100.\n",
        "2. Suggest in {language} how to make the work experience text better and stronger.\n",
        "3. Strengthen the work experience text to make it more appealing to a recruiter in {language}. \\\n",
        "Provide additional details on responsibilities and quantify results for each bullet point. \\\n",
        "Format your text as a string in {language}.\n",
        "4. Format your response as a dictionary with the following keys: \"Score__WorkExperience\", \"Comments__WorkExperience\" and \"Improvement__WorkExperience\".\n",
        "\n",
        "Work experience text: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_IMPROVE_PROJECT = \"\"\"you are given a project text delimited by triple backticks.\n",
        "1. Rate the quality of the project text by giving an integer score from 0 to 100.\n",
        "2. Suggest in {language} how to make the project text better and stronger.\n",
        "3. Strengthen the project text to make it more appealing to a recruiter in {language}, \\\n",
        "including the problem, the approach taken, the tools used and quantifiable results. \\\n",
        "Format your text as a string in {language}.\n",
        "4. Format your response as a dictionary with the following keys: Score__project, Comments__project, Improvement__project.\n",
        "\n",
        "project text: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_EVALUATE_RESUME = \"\"\"You are given a resume delimited by triple backticks.\n",
        "1. Provide an overview of the resume in {language}.\n",
        "2. Provide a comprehensive analysis of the three main strengths of the resume in {language}. \\\n",
        "Format the top 3 strengths as string containg three bullet points.\n",
        "3. Provide a comprehensive analysis of the three main weaknesses of the resume in {language}. \\\n",
        "Format the top 3 weaknesses as string containg three bullet points.\n",
        "4. Format your response as a dictionary with the following keys: resume_cv_overview, top_3_strengths, top_3_weaknesses.\n",
        "\n",
        "The strengths and weaknesses lie in the format, style and content of the resume.\n",
        "\n",
        "Resume: ```{text}```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0seglwXVvE8b"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_template(resume_sections, language=ASSISTAN_LANGUAGE):\n",
        "    \"\"\"Create the promptTemplate for selected resume sections.\n",
        "    Parameters:\n",
        "       resume_sections (list): List of resume sections from which information will be extracted.\n",
        "       language (str): the language of the AI assistant.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the final template: Add the templates from the 'templates' dictionary where keys = resume_sections\n",
        "    template = f\"\"\"For the following resume delimited by triple backticks, output in {language} the following information:\\n\\n\"\"\"\n",
        "\n",
        "    for key in resume_sections:\n",
        "        template += key + \": \" + templates[key] + \"\\n---------\\n\\n\"\n",
        "\n",
        "    template += \"For any requested information, if it is not found, output 'unknown'.\\n\\n\"\n",
        "    template += (\n",
        "        \"\"\"Format the final output as a json dictionary with the following keys: (\"\"\"\n",
        "    )\n",
        "\n",
        "    for key in resume_sections:\n",
        "        template += \"\" + key + \", \"\n",
        "    template = template[:-2] + \")\"  # remove the last \", \"\n",
        "\n",
        "    template += \"\"\"\\n\\nResume: ```{text}```\"\"\"\n",
        "\n",
        "    # Create the PromptTemplate\n",
        "    prompt_template = PromptTemplate.from_template(template)\n",
        "\n",
        "    return prompt_template"
      ],
      "metadata": {
        "id": "nL9y0F2hvLyD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = create_prompt_template(\n",
        "    ['Contact__information','CV__summary','Work__experience'],\n",
        "    language=ASSISTAN_LANGUAGE\n",
        ")\n",
        "\n",
        "print(\"**Number of tokens** in the prompt_template (instructions with no context):\",sum(tiktoken_tokens([prompt_template.template])),\"tokens.\")\n",
        "\n",
        "print(\"\\n**LLM prompt example:**\\n\")\n",
        "# Format the PromptTemplate\n",
        "prompt = prompt_template.format_prompt(text=\"...\",language=ASSISTAN_LANGUAGE).text\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "PbTU5RhGvW-c",
        "outputId": "f29bd840-8e17-4dd4-b52f-3144494c95fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Number of tokens** in the prompt_template (instructions with no context): 343 tokens.\n",
            "\n",
            "**LLM prompt example:**\n",
            "\n",
            "For the following resume delimited by triple backticks, output in english the following information:\n",
            "\n",
            "Contact__information: Extract and evaluate the contact information. Output a dictionary with the following keys:\n",
            "- candidate__name \n",
            "- candidate__title\n",
            "- candidate__location\n",
            "- candidate__email\n",
            "- candidate__phone\n",
            "- candidate__social_media: Extract a list of all social media profiles, blogs or websites.\n",
            "- evaluation__ContactInfo: Evaluate in english the contact information.\n",
            "- score__ContactInfo: Rate the contact information by giving a score (integer) from 0 to 100.\n",
            "\n",
            "---------\n",
            "\n",
            "CV__summary: Extract the summary and/or objective section. This is a separate section of the resume. If the resume doed not contain a summary and/or objective section, then simply write \"unknown\".\n",
            "---------\n",
            "\n",
            "Work__experience: Extract all work experiences. For each work experience: \n",
            "1. Extract the job title.\n",
            "2. Extract the company.  \n",
            "3. Extract the start date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
            "4. Extract the end date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
            "5. Output a dictionary with the following keys: job__title, job__company, job__start_date, job__end_date.\n",
            "\n",
            "Format your response as a list of dictionaries.\n",
            "\n",
            "---------\n",
            "\n",
            "For any requested information, if it is not found, output 'unknown'.\n",
            "\n",
            "Format the final output as a json dictionary with the following keys: (Contact__information, CV__summary, Work__experience)\n",
            "\n",
            "Resume: ```...```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def invoke_LLM(\n",
        "    llm,\n",
        "    documents,\n",
        "    resume_sections: list,\n",
        "    info_message=\"\",\n",
        "    language=ASSISTAN_LANGUAGE,\n",
        "):\n",
        "    \"\"\"Invoke LLM and get a response.\n",
        "    Parameters:\n",
        "     - llm: the LLM to call\n",
        "     - documents: the Langchain Documents.\n",
        "     - resume_sections (list): List of resume sections to be parsed.\n",
        "     - info_message (str): display an informational message.\n",
        "     - language (str): the assistant language.\n",
        "\n",
        "     Output:\n",
        "     - response_content (str): the content of the LLM response.\n",
        "     - response_tokens_count (int): count of response tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    now = (datetime.datetime.now()).strftime(\"%H:%M:%S\")\n",
        "    print(f\"**{now}** \\t{info_message}\")\n",
        "\n",
        "    # 1. Create the promptTemplate.\n",
        "    prompt_template = create_prompt_template(\n",
        "        resume_sections,\n",
        "        language=language,\n",
        "    )\n",
        "\n",
        "    # 2. Format the PromptTemplate\n",
        "    if language is not None:\n",
        "        prompt = prompt_template.format_prompt(text=documents, language=language).text\n",
        "    else:\n",
        "        prompt = prompt_template.format_prompt(text=documents).text\n",
        "\n",
        "    # 3. Invoke the LLM\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    response_content = response.content[\n",
        "        response.content.find(\"{\") : response.content.rfind(\"}\") + 1\n",
        "    ]\n",
        "    response_tokens_count = sum(tiktoken_tokens([response_content]))\n",
        "    print(f\"\"\"response tokens: {response_tokens_count}\"\"\")\n",
        "\n",
        "    return response_content, response_tokens_count"
      ],
      "metadata": {
        "id": "38p8nKP2vZ3w"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "try:\n",
        "    response_content, response_tokens_count = invoke_LLM(\n",
        "        llm,\n",
        "        documents,\n",
        "        resume_sections=[\"Contact__information\"],\n",
        "        info_message=\"Extract and evaluate contact information...\",\n",
        "        language=ASSISTAN_LANGUAGE,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Load response_content into json dictionary\n",
        "        CONTACT_INFORMATION = json.loads(response_content, strict=False)\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] json.loads returns error:\", e)\n",
        "        CONTACT_INFORMATION = {}\n",
        "\n",
        "except Exception as error:\n",
        "    print(\"[ERROR]:\", error)\n",
        "    CONTACT_INFORMATION = {}\n",
        "\n",
        "CONTACT_INFORMATION"
      ],
      "metadata": {
        "id": "Ik9NeiIAve5O",
        "outputId": "9a13b272-3b13-44a1-bc87-833ebe60da33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**04:58:11** \tExtract and evaluate contact information...\n",
            "response tokens: 157\n",
            "CPU times: user 48.1 ms, sys: 2.29 ms, total: 50.4 ms\n",
            "Wall time: 3.91 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Contact__information': {'candidate__name': 'TEJA SAJJA',\n",
              "  'candidate__title': 'unknown',\n",
              "  'candidate__location': 'San Francisco,CA',\n",
              "  'candidate__email': 'teja.sajja75@gmail.com',\n",
              "  'candidate__phone': '(571) 545-9258',\n",
              "  'candidate__social_media': ['linkedin.com/in/teja-sajja/',\n",
              "   'github.com/tejasajja'],\n",
              "  'evaluation__ContactInfo': \"The contact information is complete and easy to find. It includes the candidate's name, title, location, email, phone number, and social media profiles.\",\n",
              "  'score__ContactInfo': 100}}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_from_text(text,start_tag,end_tag=None):\n",
        "    \"\"\"Use start and end tags to extract a substring from text.\"\"\"\n",
        "    start_index = text.find(start_tag)\n",
        "    if end_tag is None:\n",
        "        extacted_txt = text[start_index+len(start_tag):]\n",
        "    else:\n",
        "        end_index = text.find(end_tag)\n",
        "        extacted_txt = text[start_index+len(start_tag):end_index]\n",
        "\n",
        "    return extacted_txt"
      ],
      "metadata": {
        "id": "GX1Zd_jAvhb-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    CONTACT_INFORMATION = json.loads(response_content+\"}\", strict=False) # Add \"}\" for error simulation\n",
        "except Exception as e:\n",
        "    print(\"[ERROR] json.loads returns error:\",e)\n",
        "\n",
        "    # Parse the response_content\n",
        "    print(\"\\n['INFO'] Parse response content...\\n\")\n",
        "    name = extract_from_text(response_content,\"\\\"candidate__name\\\": \",\"\\\"candidate__title\\\":\")\n",
        "    role = extract_from_text(response_content,\"\\\"candidate__title\\\": \",\"\\\"candidate__location\\\":\")\n",
        "    address = extract_from_text(response_content,\"\\\"candidate__location\\\": \",\"\\\"candidate__email\\\":\")\n",
        "    email_address = extract_from_text(response_content,\"\\\"candidate__email\\\": \",\"\\\"candidate__phone\\\":\")\n",
        "    phone_number = extract_from_text(response_content,\"\\\"candidate__phone\\\": \",\"\\\"candidate__social_media\\\":\")\n",
        "    social_media = extract_from_text(response_content,\"\\\"candidate__social_media\\\": \",\"\\\"evaluation__ContactInfo\\\":\")\n",
        "    eval__ContactInfo = extract_from_text(response_content,\"\\\"evaluation__ContactInfo\\\": \",\"\\\"score__ContactInfo\\\":\")\n",
        "    score__ContactInfo= extract_from_text(response_content,\"\\\"score__ContactInfo\\\": \",None)\n",
        "\n",
        "    # Create the dictionary\n",
        "    CONTACT_INFORMATION = {}\n",
        "\n",
        "    CONTACT_INFORMATION['Contact__information'] = {\n",
        "        'candidate__name': name[:name.rfind(\",\\n\")].strip()[1:-1],\n",
        "        'candidate__title': role[:role.rfind(\",\\n\")].strip()[1:-1],\n",
        "        'candidate__location': address[:address.rfind(\",\\n\")].strip()[1:-1],\n",
        "        'candidate__email': email_address[:email_address.rfind(\",\\n\")].strip()[1:-1],\n",
        "        'candidate__phone': phone_number[:phone_number.rfind(\",\\n\")].strip()[1:-1],\n",
        "        'candidate__social_media': social_media[0:social_media.rfind(\"}\\n\")-1][1:-1].strip(),\n",
        "        'evaluation__ContactInfo': eval__ContactInfo[0:eval__ContactInfo.rfind(\"}\\n\")-1][1:-1].strip(),\n",
        "        'score__ContactInfo': score__ContactInfo[0:score__ContactInfo.rfind(\"}\\n\")-1].strip()\n",
        "\n",
        "    }\n",
        "    # Convert score to int\n",
        "    try:\n",
        "        CONTACT_INFORMATION[\"Contact__information\"][\"score__ContactInfo\"] = int(\n",
        "            CONTACT_INFORMATION[\"Contact__information\"][\"score__ContactInfo\"]\n",
        "        )\n",
        "    except:\n",
        "        CONTACT_INFORMATION[\"Contact__information\"][\"score__ContactInfo\"] = -1\n",
        "\n",
        "CONTACT_INFORMATION"
      ],
      "metadata": {
        "id": "yoNjs7QUvmNh",
        "outputId": "81314658-ab9c-4ea2-a303-8267302e8434",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ERROR] json.loads returns error: Extra data: line 15 column 2 (char 569)\n",
            "\n",
            "['INFO'] Parse response content...\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Contact__information': {'candidate__name': 'TEJA SAJJA',\n",
              "  'candidate__title': 'unknown',\n",
              "  'candidate__location': 'San Francisco,CA',\n",
              "  'candidate__email': 'teja.sajja75@gmail.com',\n",
              "  'candidate__phone': '(571) 545-9258',\n",
              "  'candidate__social_media': '\"linkedin.com/in/teja-sajja/\",\\n      \"github.com/tejasajja\"\\n    ],',\n",
              "  'evaluation__ContactInfo': 'The contact information is complete and easy to find. It includes the candidate\\'s name, title, location, email, phone number, and social media profiles.\",',\n",
              "  'score__ContactInfo': 100}}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ResponseContent_Parser(response_content,list_fields,list_rfind,list_exclude_first_car ):\n",
        "    \"\"\"This is a function to parse any response_content.\n",
        "    Parameters:\n",
        "     - response_content (str): the LLM response's content.\n",
        "     - list_fields (list): List of fields to parse.\n",
        "            A field can be a dictionary. The key of the dictionary will not be parsed.\n",
        "            Example: [{'Contact__information':['candidate__name','candidate__title','candidate__location']},\n",
        "                     'CV__summary']\n",
        "            The 'Contact__Information' field content will not be parsed in this example.\n",
        "     - list_rfind (list): To parse the content of a field, we first extract the text between this field and \\\n",
        "            the next field. Then, extract the text using the Python `rfind` command, which returns the highest index in the text \\\n",
        "            where the substring is found.\n",
        "     - list_exclude_first_car (list): Exclusion or not of the first and last characters (i.e. remove \\\")\n",
        "\n",
        "    Output:\n",
        "      - INFORMATION_dict: A dictionary, where fields are the keys and the extracted texts are the values.\n",
        "\n",
        "     \"\"\"\n",
        "    # 1. Format the list_fields as a list of tuples.\n",
        "    # Each tuple should contain the field, whether to extract information or not, and the key of the dictionary.\n",
        "    list_fields_formatted = []\n",
        "\n",
        "    for field in list_fields:\n",
        "        if type(field) is dict:\n",
        "            # The key of the dictionary will not be parsed.\n",
        "            list_fields_formatted.append((list(field.keys())[0],False,None))\n",
        "            for val in list(field.values())[0]:\n",
        "                list_fields_formatted.append((val,True,list(field.keys())[0]))\n",
        "        else:\n",
        "            list_fields_formatted.append((field,True,None))\n",
        "\n",
        "    list_fields_formatted.append((None,False,None))\n",
        "\n",
        "    # 2. Parse the response_content\n",
        "    Parsed_content = {}\n",
        "\n",
        "    for i in range(len(list_fields_formatted)-1):\n",
        "        if list_fields_formatted[i][1] is False:\n",
        "            Parsed_content[list_fields_formatted[i][0]] = {} # Initialize the dictionary\n",
        "        if list_fields_formatted[i][1]:\n",
        "            # 2.1. Extract the text between this field and the next one\n",
        "            extracted_value = extract_from_text(\n",
        "                response_content,\n",
        "                f\"\\\"{list_fields_formatted[i][0]}\\\": \",\n",
        "                f\"\\\"{list_fields_formatted[i+1][0]}\\\":\"\n",
        "            )\n",
        "\n",
        "            # 2.2. Extract the text using the Python `rfind` command, which returns the highest index in the text where the substring is found.\n",
        "            if list_rfind[i] is not None:\n",
        "                extracted_value = extracted_value[:extracted_value.rfind(list_rfind[i])].strip()\n",
        "\n",
        "            # 2.3. Remove the first and last characters (i.e. remove \\\")\n",
        "            if list_exclude_first_car[i]:\n",
        "                extracted_value = extracted_value[1:-1].strip()\n",
        "\n",
        "            # 2.4. Update the dictionary Parsed_content.\n",
        "            if list_fields_formatted[i][2] is None:\n",
        "                Parsed_content[list_fields_formatted[i][0]] = extracted_value\n",
        "            else:\n",
        "                Parsed_content[list_fields_formatted[i][2]][list_fields_formatted[i][0]] = extracted_value\n",
        "\n",
        "    return Parsed_content"
      ],
      "metadata": {
        "id": "ZZb7NFlFvomH"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of keys. A field can be a dictionary. The key of the dict will not be parsed.\n",
        "list_fields = [{'Contact__information':['candidate__name','candidate__title','candidate__location',\n",
        "                                        'candidate__email','candidate__phone','candidate__social_media',\n",
        "                                        'evaluation__ContactInfo','score__ContactInfo']}]\n",
        "# list of rfind\n",
        "list_rfind = [\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\"}\\n\"]\n",
        "\n",
        "# Exclude last and first car:\n",
        "list_exclude_first_car = [True,True,True,True,True,True,False,True,False]\n",
        "\n",
        "# Parse response content\n",
        "CONTACT_INFORMATION = ResponseContent_Parser(response_content,list_fields,list_rfind,list_exclude_first_car)\n",
        "\n",
        "# Convert score to int\n",
        "try:\n",
        "    CONTACT_INFORMATION[\"Contact__information\"][\"score__ContactInfo\"] = int(\n",
        "        CONTACT_INFORMATION[\"Contact__information\"][\"score__ContactInfo\"]\n",
        "    )\n",
        "except:\n",
        "    CONTACT_INFORMATION[\"Contact__information\"][\"score__ContactInfo\"] = -1\n",
        "\n",
        "CONTACT_INFORMATION"
      ],
      "metadata": {
        "id": "XMijBuicvsoS",
        "outputId": "6ab2ce59-63a6-4f44-ff88-02d78ecb862c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Contact__information': {'candidate__name': 'TEJA SAJJA',\n",
              "  'candidate__title': 'unknown',\n",
              "  'candidate__location': 'San Francisco,CA',\n",
              "  'candidate__email': 'teja.sajja75@gmail.com',\n",
              "  'candidate__phone': '(571) 545-9258',\n",
              "  'candidate__social_media': '[\\n      \"linkedin.com/in/teja-sajja/\",\\n      \"github.com/tejasajja\"\\n    ]',\n",
              "  'evaluation__ContactInfo': \"The contact information is complete and easy to find. It includes the candidate's name, title, location, email, phone number, and social media profiles.\",\n",
              "  'score__ContactInfo': 100}}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Extract_contact_information(llm, documents):\n",
        "    \"\"\"Extract Contact Information: Name, Title, Location, Email, Phone number and Social media profiles.\"\"\"\n",
        "\n",
        "    try:\n",
        "        response_content, response_tokens_count = invoke_LLM(\n",
        "            llm,\n",
        "            documents,\n",
        "            resume_sections=[\"Contact__information\"],\n",
        "            info_message=\"Extract and evaluate contact information...\",\n",
        "            language=ASSISTAN_LANGUAGE,\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            # Load response_content into json dictionary\n",
        "            CONTACT_INFORMATION = json.loads(response_content, strict=False)\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] json.loads returns error:\", e)\n",
        "            print(\"\\n['INFO'] Parse response content...\\n\")\n",
        "            list_fields = [{'Contact__information':\n",
        "                            ['candidate__name','candidate__title','candidate__location',\n",
        "                             'candidate__email','candidate__phone','candidate__social_media',\n",
        "                             'evaluation__ContactInfo','score__ContactInfo']\n",
        "                           }]\n",
        "            list_rfind = [\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\"}\\n\"]\n",
        "            list_exclude_first_car = [True,True,True,True,True,True,False,True,False]\n",
        "            CONTACT_INFORMATION = ResponseContent_Parser(response_content,list_fields,list_rfind,list_exclude_first_car)\n",
        "            # Convert the score to int\n",
        "            try:\n",
        "                CONTACT_INFORMATION[\"Contact__information\"][\"score__ContactInfo\"] = int(\n",
        "                    CONTACT_INFORMATION[\"Contact__information\"][\"score__ContactInfo\"]\n",
        "                )\n",
        "            except:\n",
        "                CONTACT_INFORMATION[\"Contact__information\"][\"score__ContactInfo\"] = -1\n",
        "\n",
        "    except Exception as exception:\n",
        "        print(f\"[Error] {exception}\")\n",
        "        CONTACT_INFORMATION = {\n",
        "            \"Contact__information\": {\n",
        "                \"candidate__name\": \"unknown\",\n",
        "                \"candidate__title\": \"unknown\",\n",
        "                \"candidate__location\": \"unknown\",\n",
        "                \"candidate__email\": \"unknown\",\n",
        "                \"candidate__phone\": \"unknown\",\n",
        "                \"candidate__social_media\": \"unknown\",\n",
        "                \"evaluation__ContactInfo\": \"unknown\",\n",
        "                \"score__ContactInfo\": -1,\n",
        "            }\n",
        "        }\n",
        "\n",
        "    return CONTACT_INFORMATION"
      ],
      "metadata": {
        "id": "31AMfMEgvuah"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "CONTACT_INFORMATION = Extract_contact_information(llm,documents)\n",
        "CONTACT_INFORMATION"
      ],
      "metadata": {
        "id": "ccqT9Zr-v2ZQ",
        "outputId": "76d50163-a8ca-4cbb-bcf4-382eef7ef2ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**04:59:45** \tExtract and evaluate contact information...\n",
            "response tokens: 157\n",
            "CPU times: user 18.3 ms, sys: 2.11 ms, total: 20.5 ms\n",
            "Wall time: 3.38 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Contact__information': {'candidate__name': 'TEJA SAJJA',\n",
              "  'candidate__title': 'unknown',\n",
              "  'candidate__location': 'San Francisco,CA',\n",
              "  'candidate__email': 'teja.sajja75@gmail.com',\n",
              "  'candidate__phone': '(571) 545-9258',\n",
              "  'candidate__social_media': ['linkedin.com/in/teja-sajja/',\n",
              "   'github.com/tejasajja'],\n",
              "  'evaluation__ContactInfo': \"The contact information is complete and easy to find. It includes the candidate's name, title, location, email, phone number, and social media profiles.\",\n",
              "  'score__ContactInfo': 100}}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Extract_Evaluate_Summary(llm, documents):\n",
        "    \"\"\"Extract, evaluate and strengthen the summary.\"\"\"\n",
        "\n",
        "    try:\n",
        "        response_content, response_tokens_count = invoke_LLM(\n",
        "            llm,\n",
        "            documents,\n",
        "            resume_sections=[\"CV__summary\"],\n",
        "            info_message=\"Extract and evaluate the Summary....\",\n",
        "            language=ASSISTAN_LANGUAGE,\n",
        "        )\n",
        "        print(\"summary response:\\n\", response_content)\n",
        "        try:\n",
        "            # Load response_content into json dictionary\n",
        "            SUMMARY_SECTION = json.loads(response_content, strict=False)\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] json.loads returns error:\", e)\n",
        "            print(\"\\n['INFO'] Parse response content...\\n\")\n",
        "\n",
        "            list_fields = [\"CV__summary\"]\n",
        "            list_rfind = [\"}\\n\"]\n",
        "            list_exclude_first_car = [True]\n",
        "            SUMMARY_SECTION = ResponseContent_Parser(\n",
        "                response_content, list_fields, list_rfind, list_exclude_first_car\n",
        "            )\n",
        "\n",
        "    except Exception as exception:\n",
        "        print(f\"[Error] {exception}\")\n",
        "        SUMMARY_SECTION = {\"CV__summary\": \"unknown\"}\n",
        "\n",
        "    try:\n",
        "        prompt_template = PromptTemplate.from_template(PROMPT_IMPROVE_SUMMARY)\n",
        "\n",
        "        prompt = prompt_template.format_prompt(\n",
        "            resume=documents,\n",
        "            language=ASSISTAN_LANGUAGE,\n",
        "            summary=SUMMARY_SECTION[\"CV__summary\"],\n",
        "        ).text\n",
        "\n",
        "        # Invoke LLM\n",
        "        response = llm.invoke(prompt)\n",
        "        response_content = response.content[\n",
        "            response.content.find(\"{\") : response.content.rfind(\"}\") + 1\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            SUMMARY_EVAL = {}\n",
        "            SUMMARY_EVAL[\"Summary__evaluation\"] = json.loads(response_content, strict=False)\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] json.loads returns error:\", e)\n",
        "            print(\"\\n['INFO'] Parse response content...\\n\")\n",
        "            list_fields = [\n",
        "                \"evaluation__summary\",\n",
        "                \"score__summary\",\n",
        "                \"CV__summary_enhanced\",\n",
        "            ]\n",
        "            list_rfind = [\",\\n\", \",\\n\", \"}\\n\"]\n",
        "            list_exclude_first_car = [True, False, True]\n",
        "            SUMMARY_EVAL[\"Summary__evaluation\"] = ResponseContent_Parser(\n",
        "                response_content, list_fields, list_rfind, list_exclude_first_car\n",
        "            )\n",
        "            # Convert score to int\n",
        "            try:\n",
        "                SUMMARY_EVAL[\"Summary__evaluation\"][\"score__summary\"] = int(\n",
        "                    SUMMARY_EVAL[\"Summary__evaluation\"][\"score__summary\"]\n",
        "                )\n",
        "            except:\n",
        "                SUMMARY_EVAL[\"Summary__evaluation\"][\"score__summary\"] = -1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        SUMMARY_EVAL = {\n",
        "            \"Summary__evaluation\": {\n",
        "                \"evaluation__summary\": \"unknown\",\n",
        "                \"score__summary\": -1,\n",
        "                \"CV__summary_enhanced\": \"unknown\",\n",
        "            }\n",
        "        }\n",
        "\n",
        "    SUMMARY_EVAL[\"CV__summary\"] = SUMMARY_SECTION[\"CV__summary\"]\n",
        "\n",
        "\n",
        "    return SUMMARY_EVAL"
      ],
      "metadata": {
        "id": "vJiSYbz5v4Zx"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "SUMMARY_EVAL = Extract_Evaluate_Summary(llm,documents)\n",
        "SUMMARY_EVAL"
      ],
      "metadata": {
        "id": "OwYhTTBdv7pv",
        "outputId": "b438cb38-aa11-4e1a-b617-01c2b618fc83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**05:08:57** \tExtract and evaluate the Summary....\n",
            "response tokens: 96\n",
            "summary response:\n",
            " {\n",
            "  \"CV__summary\": \"Driven computer science graduate with a focus on scalable software solutions, deep learning, and large\\nlanguage models (LLMs). Proficient in Python, React, AWS, and cloud platforms, with experience in web\\ndevelopment, machine learning, and cloud infrastructure. Expertise includes working with deep learning\\nframeworks such as TensorFlow and PyTorch, utilizing LLMs, and building solutions using RAG pipelines and\\ntransformers.\"\n",
            "}\n",
            "CPU times: user 54.8 ms, sys: 5.89 ms, total: 60.7 ms\n",
            "Wall time: 5.86 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Summary__evaluation': {'evaluation__summary': \"The summary is well-written and provides a good overview of the candidate's skills and experience. It is concise and easy to read, and it highlights the candidate's most relevant qualifications. However, it could be improved by adding more specific examples of the candidate's work experience and achievements.\",\n",
              "  'score__summary': 80,\n",
              "  'CV__summary_enhanced': 'Teja Sajja is a driven computer science graduate with 2+ years of experience in web development, machine learning, and cloud infrastructure. He is proficient in Python, React, AWS, and cloud platforms, and has expertise in working with deep learning frameworks such as TensorFlow and PyTorch. Teja has a strong track record of success in building scalable software solutions, and he is passionate about using technology to solve real-world problems. He is eager to join a team where he can use his skills to make a positive impact.'},\n",
              " 'CV__summary': 'Driven computer science graduate with a focus on scalable software solutions, deep learning, and large\\nlanguage models (LLMs). Proficient in Python, React, AWS, and cloud platforms, with experience in web\\ndevelopment, machine learning, and cloud infrastructure. Expertise includes working with deep learning\\nframeworks such as TensorFlow and PyTorch, utilizing LLMs, and building solutions using RAG pipelines and\\ntransformers.'}"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "try:\n",
        "    response_content, response_tokens_count = invoke_LLM(\n",
        "        llm,\n",
        "        documents,\n",
        "        resume_sections=[\n",
        "            \"CV__Education\",\n",
        "            \"Education__evaluation\",\n",
        "            \"CV__Languages\",\n",
        "            \"Languages__evaluation\",\n",
        "        ],\n",
        "        info_message=\"Extract and evaluate education and language sections...\",\n",
        "        language=ASSISTAN_LANGUAGE,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Load response_content into json dictionary\n",
        "        Education_Language_sections = json.loads(response_content, strict=False)\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] json.loads returns error:\", e)\n",
        "        Education_Language_sections = {}\n",
        "\n",
        "except Exception as error:\n",
        "    print(\"[ERROR]:\", error)\n",
        "    Education_Language_sections = {}\n",
        "\n",
        "Education_Language_sections"
      ],
      "metadata": {
        "id": "aJqipLJnv9Xs",
        "outputId": "b582d7a8-1b53-4247-985a-4ed8a9e52ea1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**05:00:41** \tExtract and evaluate education and language sections...\n",
            "response tokens: 177\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CV__Education': [{'edu__college': 'George Mason University',\n",
              "   'edu__degree': 'Master of Science in Computer Science',\n",
              "   'edu__start_date': 'unknown',\n",
              "   'edu__end_date': 'unknown'}],\n",
              " 'Education__evaluation': {'score__edu': 70,\n",
              "  'evaluation__edu': \"The Education section is well-structured and provides a clear overview of the candidate's educational background. It includes the name of the college, the degree earned, and the relevant coursework. However, it would be more informative if it included the start and end dates for the degree program.\"},\n",
              " 'CV__Languages': [],\n",
              " 'Languages__evaluation': {'score__language': 0,\n",
              "  'evaluation__language': 'The Languages section is not available in the provided resume.'}}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_fields = ['CV__Education',\n",
        "               {'Education__evaluation':['score__edu','evaluation__edu']},\n",
        "               'CV__Languages',\n",
        "               {'Languages__evaluation':['score__language','evaluation__language']},\n",
        "              ]\n",
        "\n",
        "list_rfind = [\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\"\\n\"]\n",
        "list_exclude_first_car = [True,True,False,True,True,True,False,True]\n",
        "\n",
        "Education_Language_sections = ResponseContent_Parser(response_content,list_fields,list_rfind,list_exclude_first_car)\n",
        "Education_Language_sections"
      ],
      "metadata": {
        "id": "ra6suAHOwFi9",
        "outputId": "336899e2-1072-44b8-ce9a-ac48d66a0b1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CV__Education': '{\\n   \"edu__college\": \"George Mason University\",\\n   \"edu__degree\": \"Master of Science in Computer Science\",\\n   \"edu__start_date\": \"unknown\",\\n   \"edu__end_date\": \"unknown\"\\n  }',\n",
              " 'Education__evaluation': {'score__edu': '70',\n",
              "  'evaluation__edu': 'The Education section is well-structured and provides a clear overview of the candidate\\'s educational background. It includes the name of the college, the degree earned, and the relevant coursework. However, it would be more informative if it included the start and end dates for the degree program.\"'},\n",
              " 'CV__Languages': '',\n",
              " 'Languages__evaluation': {'score__language': '0',\n",
              "  'evaluation__language': 'The Languages section is not available in the provided resume.\"'}}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_text_to_list_of_dicts(text,dict_keys):\n",
        "    \"\"\"Convert text to a python list of dicts.\n",
        "    Parameters:\n",
        "     - text: string containing a list of dicts\n",
        "     - dict_keys (list): the keys of the dictionary which will be returned.\n",
        "    Output:\n",
        "     - list_of_dicts (list): the list of dicts to return.\n",
        "     \"\"\"\n",
        "    list_of_dicts = []\n",
        "\n",
        "    if text!=\"\": # if non-empty list\n",
        "        text_splitted = text.split(\"},\\n\")\n",
        "        dict_keys.append(None)\n",
        "\n",
        "        for i in range(len(text_splitted)):\n",
        "            dict_i = {}\n",
        "\n",
        "            for j in range(len(dict_keys)-1):\n",
        "                key_value = extract_from_text(text_splitted[i],f\"\\\"{dict_keys[j]}\\\": \",f\"\\\"{dict_keys[j+1]}\\\": \")\n",
        "                key_value = key_value[:key_value.rfind(\",\\n\")].strip()[1:-1]\n",
        "                dict_i[dict_keys[j]] = key_value\n",
        "\n",
        "            list_of_dicts.append(dict_i) # add the dict to the list.\n",
        "\n",
        "    return list_of_dicts"
      ],
      "metadata": {
        "id": "SjY3ztKIwIM9"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "languages = Education_Language_sections['CV__Languages']\n",
        "Education_Language_sections['CV__Languages'] = convert_text_to_list_of_dicts(\n",
        "    text = languages[languages.find('[')+1:languages.rfind(\"]\")].strip(),\n",
        "    dict_keys = ['spoken__language','language__fluency']\n",
        ")\n",
        "Education_Language_sections['CV__Languages']"
      ],
      "metadata": {
        "id": "Yp3gal8MwL9S",
        "outputId": "fab11a2f-4208-44f3-edf1-6f069a769c79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "education = Education_Language_sections['CV__Education']\n",
        "Education_Language_sections['CV__Education'] = convert_text_to_list_of_dicts(\n",
        "    text = education[education.find('[')+1:education.rfind(\"]\")].strip(),\n",
        "    dict_keys = ['edu__college','edu__degree','edu__start_date','edu__end_date']\n",
        ")\n",
        "Education_Language_sections['CV__Education']"
      ],
      "metadata": {
        "id": "k_nIvbZkwOcV",
        "outputId": "73465df5-ac72-4650-a339-35178d913749",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'edu__college': 'George Mason University',\n",
              "  'edu__degree': 'Master of Science in Computer Science',\n",
              "  'edu__start_date': 'unknown',\n",
              "  'edu__end_date': 'unkno'}]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Extract_Education_Language(llm, documents):\n",
        "    \"\"\"Extract and evaluate education and language sections.\"\"\"\n",
        "\n",
        "    try:\n",
        "        response_content, response_tokens_count = invoke_LLM(\n",
        "            llm,\n",
        "            documents,\n",
        "            resume_sections=[\n",
        "                \"CV__Education\",\n",
        "                \"Education__evaluation\",\n",
        "                \"CV__Languages\",\n",
        "                \"Languages__evaluation\",\n",
        "            ],\n",
        "            info_message=\"Extract and evaluate education and language sections...\",\n",
        "            language=ASSISTAN_LANGUAGE,\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            # Load response_content into json dictionary\n",
        "            Education_Language_sections = json.loads(response_content, strict=False)\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] json.loads returns error:\", e)\n",
        "            print(\"\\n['INFO'] Parse response content...\\n\")\n",
        "\n",
        "            list_fields = [\n",
        "                \"CV__Education\",\n",
        "                {\"Education__evaluation\": [\"score__edu\", \"evaluation__edu\"]},\n",
        "                \"CV__Languages\",\n",
        "                {\"Languages__evaluation\": [\"score__language\", \"evaluation__language\"]},\n",
        "            ]\n",
        "\n",
        "            list_rfind = [\",\\n\", \",\\n\", \",\\n\", \",\\n\", \",\\n\", \",\\n\", \",\\n\", \"\\n\"]\n",
        "            list_exclude_first_car = [True, True, False, True, True, True, False, True]\n",
        "\n",
        "            Education_Language_sections = ResponseContent_Parser(response_content, list_fields, list_rfind, list_exclude_first_car)\n",
        "\n",
        "            # Convert scores to int\n",
        "            try:\n",
        "                Education_Language_sections[\"Education__evaluation\"][\"score__edu\"] = int(\n",
        "                    Education_Language_sections[\"Education__evaluation\"][\"score__edu\"]\n",
        "                )\n",
        "            except:\n",
        "                Education_Language_sections[\"Education__evaluation\"][\"score__edu\"] = -1\n",
        "\n",
        "            try:\n",
        "                Education_Language_sections[\"Languages__evaluation\"][\"score__language\"] = int(\n",
        "                    Education_Language_sections[\"Languages__evaluation\"][\"score__language\"]\n",
        "                )\n",
        "            except:\n",
        "                Education_Language_sections[\"Languages__evaluation\"][\"score__language\"] = -1\n",
        "\n",
        "            # Split languages and educational texts into a Python list of dict\n",
        "            languages = Education_Language_sections[\"CV__Languages\"]\n",
        "            Education_Language_sections[\"CV__Languages\"] = (\n",
        "                convert_text_to_list_of_dicts(\n",
        "                    text=languages[\n",
        "                        languages.find(\"[\") + 1 : languages.rfind(\"]\")\n",
        "                    ].strip(),\n",
        "                    dict_keys=[\"spoken__language\", \"language__fluency\"],\n",
        "                )\n",
        "            )\n",
        "            education = Education_Language_sections[\"CV__Education\"]\n",
        "            Education_Language_sections[\"CV__Education\"] = (\n",
        "                convert_text_to_list_of_dicts(\n",
        "                    text=education[\n",
        "                        education.find(\"[\") + 1 : education.rfind(\"]\")\n",
        "                    ].strip(),\n",
        "                    dict_keys=[\n",
        "                        \"edu__college\",\n",
        "                        \"edu__degree\",\n",
        "                        \"edu__start_date\",\n",
        "                        \"edu__end_date\",\n",
        "                    ],\n",
        "                )\n",
        "            )\n",
        "    except Exception as exception:\n",
        "        print(f\"[Error] {exception}\")\n",
        "        Education_Language_sections = {\n",
        "            \"CV__Education\": [],\n",
        "            \"Education__evaluation\": {\n",
        "                \"score__edu\": -1,\n",
        "                \"evaluation__edu\": \"unknown\"\n",
        "            },\n",
        "            \"CV__Languages\": [],\n",
        "            \"Languages__evaluation\": {\n",
        "                \"score__language\": -1,\n",
        "                \"evaluation__language\": \"unknown\",\n",
        "            },\n",
        "        }\n",
        "\n",
        "    return Education_Language_sections"
      ],
      "metadata": {
        "id": "Erht-J4SwSRE"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "Education_Language_sections = Extract_Education_Language(llm,documents)\n",
        "Education_Language_sections"
      ],
      "metadata": {
        "id": "aGS2bRGywasH",
        "outputId": "66234889-b55d-47d4-93a4-71a8889006ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**05:02:18** \tExtract and evaluate education and language sections...\n",
            "response tokens: 177\n",
            "CPU times: user 33.1 ms, sys: 3.71 ms, total: 36.8 ms\n",
            "Wall time: 3.88 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CV__Education': [{'edu__college': 'George Mason University',\n",
              "   'edu__degree': 'Master of Science in Computer Science',\n",
              "   'edu__start_date': 'unknown',\n",
              "   'edu__end_date': 'unknown'}],\n",
              " 'Education__evaluation': {'score__edu': 70,\n",
              "  'evaluation__edu': \"The Education section is well-structured and provides a clear overview of the candidate's educational background. It includes the name of the college, the degree earned, and the relevant coursework. However, it would be more informative if it included the start and end dates for the degree program.\"},\n",
              " 'CV__Languages': [],\n",
              " 'Languages__evaluation': {'score__language': 0,\n",
              "  'evaluation__language': 'The Languages section is not available in the provided resume.'}}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Extract_Skills_and_Certifications(llm, documents):\n",
        "    \"\"\"Extract Skills and certifications and evaluate these sections.\"\"\"\n",
        "\n",
        "    try:\n",
        "        response_content, response_tokens_count = invoke_LLM(\n",
        "            llm,\n",
        "            documents,\n",
        "            resume_sections=[\"candidate__skills\",\"Skills__evaluation\",\"CV__Certifications\",\"Certif__evaluation\"],\n",
        "            info_message=\"Extract and evaluate the skills and certifications...\",\n",
        "            language=ASSISTAN_LANGUAGE,\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            # Load response_content into json dictionary\n",
        "            SKILLS_and_CERTIF = json.loads(response_content, strict=False)\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] json.loads returns error:\", e)\n",
        "            print(\"\\n['INFO'] Parse response content...\\n\")\n",
        "\n",
        "            skills = extract_from_text(response_content,\"\\\"candidate__skills\\\": \", \"\\\"Skills__evaluation\\\":\")\n",
        "            skills = skills.replace(\"\\n  \",\"\\n\").replace(\"],\\n\",\"\").replace(\"[\\n\",\"\")\n",
        "            score_skills = extract_from_text(response_content,\"\\\"score__skills\\\": \", \"\\\"evaluation__skills\\\":\")\n",
        "            evaluation_skills = extract_from_text(response_content,\"\\\"evaluation__skills\\\": \", \"\\\"CV__Certifications\\\":\")\n",
        "\n",
        "            certif_text = extract_from_text(response_content,\"\\\"CV__Certifications\\\": \", \"\\\"Certif__evaluation\\\":\")\n",
        "            certif_score = extract_from_text(response_content,\"\\\"score__certif\\\": \", \"\\\"evaluation__certif\\\":\")\n",
        "            certif_eval = extract_from_text(response_content,\"\\\"evaluation__certif\\\": \", None)\n",
        "\n",
        "\n",
        "            # Create the dictionary\n",
        "            SKILLS_and_CERTIF = {}\n",
        "            SKILLS_and_CERTIF[\"candidate__skills\"] = [skill.strip()[1:-1] for skill in skills.split(\",\\n\")]\n",
        "\n",
        "            # Convert the score to int\n",
        "            try:\n",
        "                score_skills_int = int(score_skills[0 : score_skills.rfind(\",\\n\")])\n",
        "            except:\n",
        "                score_skills_int = -1\n",
        "\n",
        "            SKILLS_and_CERTIF[\"Skills__evaluation\"] = {\n",
        "                \"score__skills\": score_skills_int,\n",
        "                \"evaluation__skills\": evaluation_skills[: evaluation_skills.rfind(\"}\\n\")].strip()[1:-1],\n",
        "            }\n",
        "\n",
        "            # Convert text to list of dictionaries\n",
        "            list_certifs = convert_text_to_list_of_dicts(\n",
        "                text=certif_text[certif_text.find(\"[\") + 1 : certif_text.rfind(\"]\")].strip(),\n",
        "                dict_keys=[\n",
        "                    \"certif__title\",\n",
        "                    \"certif__organization\",\n",
        "                    \"certif__date\",\n",
        "                    \"certif__expiry_date\",\n",
        "                    \"certif__details\",\n",
        "                ],\n",
        "            )\n",
        "            SKILLS_and_CERTIF[\"CV__Certifications\"] = list_certifs\n",
        "            try:\n",
        "                certif_score_int = int(certif_score[0 : certif_score.rfind(\",\\n\")])\n",
        "            except:\n",
        "                certif_score_int = -1\n",
        "            SKILLS_and_CERTIF[\"Certif__evaluation\"] = {\n",
        "                \"score__certif\": certif_score_int,\n",
        "                \"evaluation__certif\": certif_eval[: certif_eval.rfind(\"}\\n\")].strip()[1:-1],\n",
        "            }\n",
        "\n",
        "    except Exception as exception:\n",
        "        SKILLS_and_CERTIF = {\n",
        "            \"candidate__skills\": [],\n",
        "            \"Skills__evaluation\": {\n",
        "                \"score__skills\": -1,\n",
        "                \"evaluation__skills\": \"unknown\",\n",
        "            },\n",
        "            \"CV__Certifications\": [],\n",
        "            \"Certif__evaluation\": {\n",
        "                \"score__certif\": -1,\n",
        "                \"evaluation__certif\": \"unknown\",\n",
        "            },\n",
        "        }\n",
        "        print(f\"[Error] {exception}\")\n",
        "\n",
        "    return SKILLS_and_CERTIF"
      ],
      "metadata": {
        "id": "OIhQ0Qn6wcSW"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "SKILLS_and_CERTIF = Extract_Skills_and_Certifications(llm,documents)\n",
        "SKILLS_and_CERTIF"
      ],
      "metadata": {
        "id": "2mcVwhAxwe3L",
        "outputId": "7a51102a-62d0-4523-e051-296fc5f0608c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**05:02:28** \tExtract and evaluate the skills and certifications...\n",
            "response tokens: 630\n",
            "CPU times: user 71.7 ms, sys: 2.57 ms, total: 74.3 ms\n",
            "Wall time: 9.99 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'candidate__skills': ['Python',\n",
              "  'Java',\n",
              "  'C',\n",
              "  'C++',\n",
              "  'JavaScript',\n",
              "  'TypeScript',\n",
              "  'Unix',\n",
              "  'Linux',\n",
              "  'SQL',\n",
              "  'React',\n",
              "  'Next.js',\n",
              "  'Spring boot',\n",
              "  'Node.js',\n",
              "  'HTML/DOM',\n",
              "  'Django',\n",
              "  'Spring-boot',\n",
              "  'FastAPI',\n",
              "  'CICD',\n",
              "  'AWS (EC 2, Lambda, S3, RDS, ELB, API Gateway, Route 53)',\n",
              "  'Azure (Blob Storage, Functions, Logic Apps)',\n",
              "  'SQL',\n",
              "  'MySQL',\n",
              "  'PLSQL',\n",
              "  'PostgreSQL',\n",
              "  'AWS Redshift',\n",
              "  'MongoDB',\n",
              "  'TensorFlow',\n",
              "  'PyTorch(CNN)',\n",
              "  '(R-CNN)',\n",
              "  'Transformers',\n",
              "  'Attention',\n",
              "  'LLM',\n",
              "  'RAG Pipeline',\n",
              "  'BERT',\n",
              "  'PySpark',\n",
              "  'Databricks',\n",
              "  'Scikit-Learn',\n",
              "  'Azure ML Studio',\n",
              "  'Airflow',\n",
              "  'Git',\n",
              "  'Eclipse',\n",
              "  'VS Code',\n",
              "  'Docker',\n",
              "  'Kubernetes',\n",
              "  'Jenkins',\n",
              "  'Azure DevOps',\n",
              "  'Apache Kafka',\n",
              "  'Maven'],\n",
              " 'Skills__evaluation': {'score__skills': 90,\n",
              "  'evaluation__skills': \"The Skills section is comprehensive and well-organized, providing a clear overview of the candidate's technical proficiencies. It includes a wide range of hard and soft skills, demonstrating the candidate's versatility and adaptability. The skills are listed in a structured manner, making it easy for recruiters to quickly identify the candidate's strengths and areas of expertise.\"},\n",
              " 'CV__Certifications': [{'certif__title': 'Microsoft Certified: Azure Data Scientist Associate (DP-100)',\n",
              "   'certif__organization': 'Microsoft',\n",
              "   'certif__date': 'unknown',\n",
              "   'certif__expiry_date': 'unknown',\n",
              "   'certif__details': 'unknown'},\n",
              "  {'certif__title': 'Oracle Certified Professional: Oracle Generative AI Specialist (OCP-GAI)',\n",
              "   'certif__organization': 'Oracle',\n",
              "   'certif__date': 'unknown',\n",
              "   'certif__expiry_date': 'unknown',\n",
              "   'certif__details': 'unknown'}],\n",
              " 'Certif__evaluation': {'score__certif': 70,\n",
              "  'evaluation__certif': \"The candidate has two certifications, both of which are relevant to the field of computer science. The Microsoft Certified: Azure Data Scientist Associate (DP-100) certification demonstrates the candidate's proficiency in Azure data science technologies, while the Oracle Certified Professional: Oracle Generative AI Specialist (OCP-GAI) certification demonstrates the candidate's knowledge of Oracle's generative AI technologies. Overall, the certifications provide evidence of the candidate's commitment to professional development and their expertise in the field.\"}}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Extract_PROFESSIONAL_EXPERIENCE(llm, documents):\n",
        "    \"\"\"Extract the list of work experience and projects.\"\"\"\n",
        "\n",
        "    try:\n",
        "        response_content, response_tokens_count = invoke_LLM(\n",
        "            llm,\n",
        "            documents,\n",
        "            resume_sections=[\"Work__experience\", \"CV__Projects\"],\n",
        "            info_message=\"Extract list of work experience and projects...\",\n",
        "            language=ASSISTAN_LANGUAGE,\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            # Load response_content into json dictionary\n",
        "            PROFESSIONAL_EXPERIENCE = json.loads(response_content+\"}\", strict=False)\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] json.loads returns error:\", e)\n",
        "            print(\"\\n['INFO'] Parse response content...\\n\")\n",
        "            work_experiences = extract_from_text(response_content, '\"Work__experience\": ', '\"CV__Projects\":')\n",
        "            projects = extract_from_text(response_content, '\"CV__Projects\": ', None)\n",
        "\n",
        "            # Create the dictionary\n",
        "            PROFESSIONAL_EXPERIENCE = {}\n",
        "            PROFESSIONAL_EXPERIENCE[\"Work__experience\"] = convert_text_to_list_of_dicts(\n",
        "                text=work_experiences[work_experiences.find(\"[\") + 1 : work_experiences.rfind(\"]\")].strip()[1:-1],\n",
        "                dict_keys=[\"job__title\", \"job__company\", \"job__start_date\", \"job__end_date\"],\n",
        "            )\n",
        "            PROFESSIONAL_EXPERIENCE[\"CV__Projects\"] = convert_text_to_list_of_dicts(\n",
        "                text=projects[projects.find(\"[\") + 1 : projects.rfind(\"]\")].strip()[1:-1],\n",
        "                dict_keys=[\"project__title\", \"project__start_date\", \"project__end_date\"],\n",
        "            )\n",
        "        # delete 'unknown' projects and work experience\n",
        "        try:\n",
        "            for work_experience in PROFESSIONAL_EXPERIENCE[\"Work__experience\"]:\n",
        "                if work_experience[\"job__title\"] == \"unknown\":\n",
        "                    PROFESSIONAL_EXPERIENCE[\"Work__experience\"].remove(work_experience)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "        try:\n",
        "            for project in PROFESSIONAL_EXPERIENCE[\"CV__Projects\"]:\n",
        "                if project[\"project__title\"] == \"unknown\":\n",
        "                    PROFESSIONAL_EXPERIENCE[\"CV__Projects\"].remove(project)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    except Exception as exception:\n",
        "        PROFESSIONAL_EXPERIENCE = {\n",
        "            \"Work__experience\": [],\n",
        "            \"CV__Projects\": []\n",
        "        }\n",
        "        print(exception)\n",
        "\n",
        "    return PROFESSIONAL_EXPERIENCE"
      ],
      "metadata": {
        "id": "fWkXn6tywgKj"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "PROFESSIONAL_EXPERIENCE = Extract_PROFESSIONAL_EXPERIENCE(llm,documents)\n",
        "PROFESSIONAL_EXPERIENCE"
      ],
      "metadata": {
        "id": "aPqUaHSnwivY",
        "outputId": "8b8053ba-e897-41cd-903c-0790daf5c9e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**05:02:45** \tExtract list of work experience and projects...\n",
            "response tokens: 280\n",
            "[ERROR] json.loads returns error: Extra data: line 38 column 2 (char 980)\n",
            "\n",
            "['INFO'] Parse response content...\n",
            "\n",
            "CPU times: user 44 ms, sys: 2.32 ms, total: 46.3 ms\n",
            "Wall time: 5.1 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Work__experience': [{'job__title': 'Graduate Teaching Assistant',\n",
              "   'job__company': 'George Mason University, Virginia',\n",
              "   'job__start_date': 'Aug 2023',\n",
              "   'job__end_date': 'Dec 2023'},\n",
              "  {'job__title': 'Engineering Intern',\n",
              "   'job__company': 'Celigo, Hyderabad',\n",
              "   'job__start_date': 'Sep 2021',\n",
              "   'job__end_date': 'Mar 2022'}],\n",
              " 'CV__Projects': [{'project__title': 'Automated Video Translation and Voice Synthesis',\n",
              "   'project__start_date': 'unknown',\n",
              "   'project__end_date': 'unknown'},\n",
              "  {'project__title': 'Intelligent Resume Optimization System Using LLMs',\n",
              "   'project__start_date': 'unknown',\n",
              "   'project__end_date': 'unknown'},\n",
              "  {'project__title': 'Interactive Whiteboard Drawing Application',\n",
              "   'project__start_date': 'unknown',\n",
              "   'project__end_date': 'unknown'},\n",
              "  {'project__title': 'Web App Deployment on AWS with Kubernetes',\n",
              "   'project__start_date': 'unknown',\n",
              "   'project__end_date': 'unknown'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relevant_documents(query,documents,retriever):\n",
        "    \"\"\"Retreieve most relevant documents from Langchain documents using the CoherRerank retriever.\"\"\"\n",
        "\n",
        "    # 1.1. Get relevant documents using the CohereRerank retriever\n",
        "\n",
        "    retrieved_docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "    # 1.2. Keep only relevant documents where (relevance_score >= (max(relevance_scores) - 0.1))\n",
        "\n",
        "    relevance_scores = [\n",
        "        retrieved_docs[j].metadata[\"relevance_score\"]\n",
        "        for j in range(len(retrieved_docs))\n",
        "    ]\n",
        "    max_relevance_score = max(relevance_scores)\n",
        "    threshold = max_relevance_score - 0.1\n",
        "\n",
        "    relevant_doc_ids = []\n",
        "\n",
        "    for j in range(len(retrieved_docs)):\n",
        "\n",
        "        # Keep relevant documents with (relevance_score >= threshold)\n",
        "        if retrieved_docs[j].metadata[\"relevance_score\"] >= threshold:\n",
        "            relevant_doc_ids.append(retrieved_docs[j].metadata[\"doc_number\"])\n",
        "\n",
        "    # Append the next document to the most relevant document, as relevant information may be split between two documents.\n",
        "    relevant_doc_ids.append(min(relevant_doc_ids[0]+1, len(documents)-1))\n",
        "\n",
        "    relevant_doc_ids = sorted(set(relevant_doc_ids))  # Sort doc ids\n",
        "\n",
        "    # get the most relevant documents (+ next document)\n",
        "    relevant_documents = [\n",
        "        documents[k] for k in relevant_doc_ids\n",
        "    ]\n",
        "\n",
        "    return relevant_documents"
      ],
      "metadata": {
        "id": "6c6qw8SEwkS1"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Extract_Job_Responsibilities(llm, documents, retriever, PROFESSIONAL_EXPERIENCE):\n",
        "    \"\"\"Extract job responsibilities for each job in PROFESSIONAL_EXPERIENCE.\"\"\"\n",
        "\n",
        "    now = (datetime.datetime.now()).strftime(\"%H:%M:%S\")\n",
        "    print(f\"**{now}** \\tExtract work experience responsabilities...\")\n",
        "\n",
        "    for i in range(len(PROFESSIONAL_EXPERIENCE[\"Work__experience\"])):\n",
        "        try:\n",
        "            Work_experience_i = PROFESSIONAL_EXPERIENCE[\"Work__experience\"][i]\n",
        "            print(f\"\\n\\n{i}: {Work_experience_i['job__title']}\", end=\" | \")\n",
        "\n",
        "            # 1. Query\n",
        "            query = f\"\"\"Extract from the resume delimited by triple backticks \\\n",
        "all the duties and responsabilities of the following work experience: \\\n",
        "(title = '{Work_experience_i['job__title']}'\"\"\"\n",
        "            if str(Work_experience_i[\"job__company\"]) != \"unknown\":\n",
        "                query += f\" and company = '{Work_experience_i['job__company']}'\"\n",
        "            if str(Work_experience_i[\"job__start_date\"]) != \"unknown\":\n",
        "                query += f\" and start date = '{Work_experience_i['job__start_date']}'\"\n",
        "            if str(Work_experience_i[\"job__end_date\"]) != \"unknown\":\n",
        "                query += f\" and end date = '{Work_experience_i['job__end_date']}'\"\n",
        "            query += \")\\n\"\n",
        "\n",
        "            # 2. For longer CVs (i.e. number of documents > 2),\n",
        "            # use the CohereRerank retriever to find the most relevant documents.\n",
        "            if len(documents)>2:\n",
        "                try:\n",
        "                    relevant_documents = get_relevant_documents(query, documents, retriever)\n",
        "                except Exception as err:\n",
        "                    print(f\"[ERROR] get_relevant_documents error: {err}\")\n",
        "                    relevant_documents = documents\n",
        "            else:\n",
        "                relevant_documents = documents\n",
        "\n",
        "            print(f\"relevant docs : {len(relevant_documents)}\", end=\" | \")\n",
        "\n",
        "            # 3. Invoke the LLM\n",
        "            prompt = (\n",
        "                query\n",
        "                + f\"\"\"Output the duties in a json dictionary with the following keys (__duty_id__,__duty__). \\\n",
        "Use this format: \"1\":\"duty\",\"2\":\"another duty\".\n",
        "Resume:\\n\\n ```{relevant_documents}```\"\"\"\n",
        "            )\n",
        "\n",
        "            print(f\"prompt tokens: {sum(tiktoken_tokens([prompt]))}\", end=\" | \")\n",
        "\n",
        "            response = llm.invoke(prompt)\n",
        "            response_content = response.content[response.content.find(\"{\") : response.content.rfind(\"}\") + 1]\n",
        "            print(f\"\"\"response tokens: {sum(tiktoken_tokens([response_content]))}\"\"\")\n",
        "\n",
        "            try:\n",
        "                # 4. Convert the response content to json dict and update work_experience\n",
        "                Work_experience_i[\"work__duties\"] = json.loads(response_content, strict=False)\n",
        "            except Exception as e:\n",
        "                print(\"\\n[ERROR] json.loads returns error:\", e, \"\\n\\n\")\n",
        "                print(\"\\n['INFO'] Parse response content...\\n\")\n",
        "                Work_experience_i[\"work__duties\"] = {}\n",
        "                list_duties = (\n",
        "                    response_content[response_content.find(\"{\") + 1 : response_content.rfind(\"}\")].strip().split(\",\\n\")\n",
        "                )\n",
        "                for j in range(len(list_duties)):\n",
        "                    try:\n",
        "                        Work_experience_i[\"work__duties\"][f\"{j+1}\"] = (list_duties[j].split('\":')[1].strip()[1:-1])\n",
        "                    except:\n",
        "                        Work_experience_i[\"work__duties\"][f\"{j+1}\"] = \"unknown\"\n",
        "\n",
        "        except Exception as exception:\n",
        "            print(f\"[ERROR] {exception}\")\n",
        "            Work_experience_i[\"work__duties\"] = {}\n",
        "\n",
        "    return PROFESSIONAL_EXPERIENCE"
      ],
      "metadata": {
        "id": "TS3pmymdwp30"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "PROFESSIONAL_EXPERIENCE = Extract_Job_Responsibilities(llm,documents,retriever,PROFESSIONAL_EXPERIENCE)\n",
        "PROFESSIONAL_EXPERIENCE['Work__experience']"
      ],
      "metadata": {
        "id": "12Z9NwmKwsMT",
        "outputId": "156dfe5d-45c5-46bf-a7a5-40574044ec7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**05:03:23** \tExtract work experience responsabilities...\n",
            "\n",
            "\n",
            "0: Graduate Teaching Assistant | relevant docs : 2 | prompt tokens: 1384 | response tokens: 170\n",
            "\n",
            "\n",
            "1: Engineering Intern | relevant docs : 2 | prompt tokens: 1381 | response tokens: 112\n",
            "CPU times: user 43.5 ms, sys: 6.49 ms, total: 50 ms\n",
            "Wall time: 5.61 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'job__title': 'Graduate Teaching Assistant',\n",
              "  'job__company': 'George Mason University, Virginia',\n",
              "  'job__start_date': 'Aug 2023',\n",
              "  'job__end_date': 'Dec 2023',\n",
              "  'work__duties': {'1': 'Coordinated and guided a class of 65 students in the Applied Machine Learning course',\n",
              "   '2': 'Crafted and presented materials aligned with the curriculum, covering key machine learning concepts such as supervised and unsupervised learning, regression, classification, clustering, and dimensionality reduction, ensuring students built a strong foundation in these essential topics',\n",
              "   '3': 'Designed, administered, and assessed assignments, projects, quizzes, and exams, which helped students develop a strong understanding of the material',\n",
              "   '4': 'Created problem sets and coding assignments focused on regression, classification, clustering, and dimensionality reduction',\n",
              "   '5': 'Hosted a leaderboard website where students submit their results on a provided test set, allowing them to compete by comparing their model performance on shared datasets, promoting both collaboration and competition'}},\n",
              " {'job__title': 'Engineering Intern',\n",
              "  'job__company': 'Celigo, Hyderabad',\n",
              "  'job__start_date': 'Sep 2021',\n",
              "  'job__end_date': 'Mar 2022',\n",
              "  'work__duties': {'1': 'Collaborated with a team of Software Associates to streamline data workflows using Postman, integrator.io, JavaScript, and Python, improving efficiency and integration capabilities.',\n",
              "   '2': 'Developed and integrated APIs on integrator.io which automate workflows between multiple applications, including CRM systems like NetSuite and Salesforce, enhancing data interoperability.',\n",
              "   '3': 'Built and deployed multiple API endpoints on integrator.io, customizing functionalities based on specific workflows and endpoints, which were later used by customers for seamless API integrations.'}}]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Extract_Project_Details(llm, documents, retriever, PROFESSIONAL_EXPERIENCE):\n",
        "    \"\"\"Extract project details for each project in PROFESSIONAL_EXPERIENCE.\"\"\"\n",
        "\n",
        "    now = (datetime.datetime.now()).strftime(\"%H:%M:%S\")\n",
        "    print(f\"**{now}** \\tExtract project details...\")\n",
        "\n",
        "    for i in range(len(PROFESSIONAL_EXPERIENCE[\"CV__Projects\"])):\n",
        "        try:\n",
        "            project_i = PROFESSIONAL_EXPERIENCE[\"CV__Projects\"][i]\n",
        "            print(f\"{i}: {project_i['project__title']}\", end=\" | \")\n",
        "\n",
        "            # 1. Extract relevant documents\n",
        "            query = f\"\"\"Extract from the resume (delimited by triple backticks) what is listed about the following project: \\\n",
        "(project title = '{project_i['project__title']}'\"\"\"\n",
        "            if str(project_i[\"project__start_date\"]) != \"unknown\":\n",
        "                query += f\" and start date = '{project_i['project__start_date']}'\"\n",
        "            if str(project_i[\"project__end_date\"]) != \"unknown\":\n",
        "                query += f\" and end date = '{project_i['project__end_date']}'\"\n",
        "            query += \")\"\n",
        "\n",
        "            if len(documents)>2:\n",
        "                try:\n",
        "                    relevant_documents = get_relevant_documents(query, documents, retriever)\n",
        "                except Exception as err:\n",
        "                    print(f\"[ERROR] get_relevant_documents error: {err}\")\n",
        "                    relevant_documents = documents\n",
        "            else:\n",
        "                relevant_documents = documents\n",
        "\n",
        "            print(f\"relevant docs : {len(relevant_documents)}\", end=\" | \")\n",
        "\n",
        "            # 2. Invoke the LLM\n",
        "\n",
        "            prompt = (query + f\"\"\"Format the extracted text into a string (with bullet points).\n",
        "Resume:\\n\\n ```{relevant_documents}```\"\"\" )\n",
        "\n",
        "            print(f\"prompt tokens: {sum(tiktoken_tokens([prompt]))}\", end=\" | \")\n",
        "\n",
        "            response = llm.invoke(prompt)\n",
        "\n",
        "            response_content = response.content\n",
        "            project_i[\"project__description\"] = response_content\n",
        "            print(f\"\"\"response tokens: {sum(tiktoken_tokens([response_content]))}\"\"\")\n",
        "\n",
        "        except Exception as exception:\n",
        "            project_i[\"project__description\"] = \"unknown\"\n",
        "            print(exception)\n",
        "\n",
        "    return PROFESSIONAL_EXPERIENCE"
      ],
      "metadata": {
        "id": "LixpIC2bwtzE"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "PROFESSIONAL_EXPERIENCE = Extract_Project_Details(llm,documents,retriever,PROFESSIONAL_EXPERIENCE)\n",
        "PROFESSIONAL_EXPERIENCE['CV__Projects']"
      ],
      "metadata": {
        "id": "mA-0t2JXwxPH",
        "outputId": "380be305-1cbb-4046-8e6a-f719aed6a0f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**05:03:46** \tExtract project details...\n",
            "0: Automated Video Translation and Voice Synthesis | relevant docs : 2 | prompt tokens: 1336 | response tokens: 88\n",
            "1: Intelligent Resume Optimization System Using LLMs | relevant docs : 2 | prompt tokens: 1337 | response tokens: 74\n",
            "2: Interactive Whiteboard Drawing Application | relevant docs : 2 | prompt tokens: 1333 | response tokens: 58\n",
            "3: Web App Deployment on AWS with Kubernetes | relevant docs : 2 | prompt tokens: 1335 | response tokens: 86\n",
            "CPU times: user 76.2 ms, sys: 16.8 ms, total: 93 ms\n",
            "Wall time: 6.87 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'project__title': 'Automated Video Translation and Voice Synthesis',\n",
              "  'project__start_date': 'unknown',\n",
              "  'project__end_date': 'unknown',\n",
              "  'project__description': '- Implemented a script to translate video content from one language to another while maintaining natural narration flow and synchronization\\n- Integrated WhisperX for precise audio transcription, Bark for high-quality voice synthesis, and additional python tools for comprehensive video and audio processing tasks, including extraction, merging, and encoding\\n- Focused on improving synchronization for seamless video integration, expanding language support, refining contextual translation using GPT, and exploring user interface development and voice cloning techniques'},\n",
              " {'project__title': 'Intelligent Resume Optimization System Using LLMs',\n",
              "  'project__start_date': 'unknown',\n",
              "  'project__end_date': 'unknown',\n",
              "  'project__description': '- Developed an innovative tool to analyze and customize resumes based on specific job descriptions.\\n- Provided scores and suggestions to enhance resume match with job descriptions.\\n- Implemented a Retrieval-Augmented Generation (RAG) pipeline to scan and analyze resume, enhancing it for new job Descriptions.\\n- Utilized Cohere and Google Gemini with LangChain for accurate skill and keyword extraction.'},\n",
              " {'project__title': 'Interactive Whiteboard Drawing Application',\n",
              "  'project__start_date': 'unknown',\n",
              "  'project__end_date': 'unknown',\n",
              "  'project__description': '- Developed a Next.js -based whiteboard app integrating MediaPipe for hand gesture recognition\\n- Enabled intuitive drawing through finger movement tracking\\n- Optimized hand tracking for accurate, responsive drawing experiences\\n- Designed a minimalist UI, offering essential drawing tools for a straightforward and effective digital sketching experience'},\n",
              " {'project__title': 'Web App Deployment on AWS with Kubernetes',\n",
              "  'project__start_date': 'unknown',\n",
              "  'project__end_date': 'unknown',\n",
              "  'project__description': '- Led development of a microservices-based web application using Java, Spring Boot, Docker, Kubernetes, AWS, and MySQL.\\n- Engineered JWT token-based authentication for secure web service APIs.\\n- Utilized AWS RDS for efficient data storage and retrieval.\\n- Containerized the application using Docker and deployed on Kubernetes for scalability and resiliency.\\n- Established a CI/CD pipeline with GitHub and Jenkins for automated build and deployment.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_IMPROVE_WORK_EXPERIENCE = \"\"\"you are given a work experience text delimited by triple backticks.\n",
        "1. Rate the quality of the work experience text by giving an integer score from 0 to 100.\n",
        "2. Suggest in {language} how to make the work experience text better and stronger.\n",
        "3. Strengthen the work experience text to make it more appealing to a recruiter in {language}. \\\n",
        "Provide additional details on responsibilities and quantify results for each bullet point. \\\n",
        "Format the output as a string.\n",
        "4. Output a json object that contains the following keys: Score__WorkExperience, Comments__WorkExperience, Improvement__WorkExperience.\n",
        "\n",
        "Work experience text: ```{text}```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dnmJUvjhwzQD"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def improve_text_quality(PROMPT,text_to_imporve,llm,language=ASSISTAN_LANGUAGE):\n",
        "    \"\"\"Invoke LLM to improve the text quality.\"\"\"\n",
        "    query = PROMPT.format(text=text_to_imporve,language=language)\n",
        "    response = llm.invoke(query)\n",
        "    return response"
      ],
      "metadata": {
        "id": "6vcGwRZIw07d"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def improve_work_experience(WORK_EXPERIENCE:list,llm):\n",
        "    \"\"\"Use LLM to improve each bullet point in the work experience responsibilities.\"\"\"\n",
        "\n",
        "    print(f\"Improve text quality of work Experience...\\n\")\n",
        "\n",
        "    # Call LLM for any work experience to get a better and stronger text.\n",
        "    for i in range(len(WORK_EXPERIENCE)):\n",
        "\n",
        "        WORK_EXPERIENCE_i = WORK_EXPERIENCE[i]\n",
        "\n",
        "        print(f\"{i}: {WORK_EXPERIENCE_i['job__title']}\",end=\" | \")\n",
        "\n",
        "        # 1. Convert the duties from dict to string\n",
        "\n",
        "        text_duties = \"\"\n",
        "        for duty in list(WORK_EXPERIENCE_i['work__duties'].values()):\n",
        "            text_duties += \"- \" + duty\n",
        "\n",
        "        # 2. Invoke the LLM\n",
        "\n",
        "        response = improve_text_quality(PROMPT_IMPROVE_WORK_EXPERIENCE,text_duties,llm)\n",
        "        response_content = response.content\n",
        "\n",
        "        # 3. Convert response content to json dict with keys:\n",
        "        # ('Score__WorkExperience','Comments__WorkExperience','Improvement__WorkExperience')\n",
        "\n",
        "        response_content = response_content[response_content.find(\"{\"):response_content.rfind(\"}\")+1]\n",
        "        print(f\"\"\"response tokens: {sum(tiktoken_tokens([response_content]))}\"\"\", end=\"\\n\")\n",
        "\n",
        "        try:\n",
        "            list_fields = ['Score__WorkExperience','Comments__WorkExperience','Improvement__WorkExperience']\n",
        "            list_rfind = [\",\\n\",\",\\n\",\"\\n\"]\n",
        "            list_exclude_first_car = [False,True,True]\n",
        "            response_content_dict = ResponseContent_Parser(response_content,list_fields,list_rfind,list_exclude_first_car)\n",
        "            try:\n",
        "                response_content_dict['Score__WorkExperience'] = int(response_content_dict['Score__WorkExperience'])\n",
        "            except:\n",
        "                response_content_dict['Score__WorkExperience'] = -1\n",
        "\n",
        "        except Exception as e:\n",
        "            response_content_dict = {\n",
        "                'Score__WorkExperience':-1,\n",
        "                'Comments__WorkExperience':\"\",\n",
        "                'Improvement__WorkExperience':\"\"}\n",
        "            print(e)\n",
        "\n",
        "\n",
        "        # 4. Update PROFESSIONAL_EXPERIENCE dictionary:\n",
        "\n",
        "        WORK_EXPERIENCE_i['Score__WorkExperience'] = response_content_dict['Score__WorkExperience']\n",
        "        WORK_EXPERIENCE_i['Comments__WorkExperience'] = response_content_dict['Comments__WorkExperience']\n",
        "        WORK_EXPERIENCE_i['Improvement__WorkExperience'] = response_content_dict['Improvement__WorkExperience']\n",
        "\n",
        "    return WORK_EXPERIENCE"
      ],
      "metadata": {
        "id": "CAvK2nfWw290"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "PROFESSIONAL_EXPERIENCE['Work__experience'] = improve_work_experience(PROFESSIONAL_EXPERIENCE['Work__experience'],llm)\n",
        "\n",
        "PROFESSIONAL_EXPERIENCE['Work__experience']"
      ],
      "metadata": {
        "id": "9pi_Mxmrw7CY",
        "outputId": "0864487b-32bd-4dae-9d17-370a957929e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Improve text quality of work Experience...\n",
            "\n",
            "0: Graduate Teaching Assistant | response tokens: 267\n",
            "1: Engineering Intern | response tokens: 213\n",
            "CPU times: user 95.3 ms, sys: 12.1 ms, total: 107 ms\n",
            "Wall time: 15.8 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'job__title': 'Graduate Teaching Assistant',\n",
              "  'job__company': 'George Mason University, Virginia',\n",
              "  'job__start_date': 'Aug 2023',\n",
              "  'job__end_date': 'Dec 2023',\n",
              "  'work__duties': {'1': 'Coordinated and guided a class of 65 students in the Applied Machine Learning course',\n",
              "   '2': 'Crafted and presented materials aligned with the curriculum, covering key machine learning concepts such as supervised and unsupervised learning, regression, classification, clustering, and dimensionality reduction, ensuring students built a strong foundation in these essential topics',\n",
              "   '3': 'Designed, administered, and assessed assignments, projects, quizzes, and exams, which helped students develop a strong understanding of the material',\n",
              "   '4': 'Created problem sets and coding assignments focused on regression, classification, clustering, and dimensionality reduction',\n",
              "   '5': 'Hosted a leaderboard website where students submit their results on a provided test set, allowing them to compete by comparing their model performance on shared datasets, promoting both collaboration and competition'},\n",
              "  'Score__WorkExperience': 60,\n",
              "  'Comments__WorkExperience': '- The work experience text is well-written and provides a good overview of the responsibilities and accomplishments in the role.\\\\n- However, the text could be improved by adding more details about the responsibilities and accomplishments in each bullet point.\\\\n- The text could also use more specific and quantifiable language.',\n",
              "  'Improvement__WorkExperience': '- Coordinated and guided a class of 65 students in the Applied Machine Learning course, ensuring a comprehensive understanding of key machine learning concepts.\\\\n- Crafted and presented materials aligned with the curriculum, covering supervised and unsupervised learning, regression, classification, clustering, and dimensionality reduction, resulting in a strong foundation for students.\\\\n- Designed, administered, and assessed assignments, projects, quizzes, and exams, which helped students develop a strong understanding of the material, as evidenced by high exam scores and project evaluations.\\\\n- Created problem sets and coding assignments focused on regression, classification, clustering, and dimensionality reduction, which challenged students and reinforced their learning.\\\\n- Hosted a leaderboard website where students submitted their results on a provided test set, allowing them to compete by comparing their model performance on shared datasets, promoting both collaboration and competition, resulting in increased student engagement and improved model performance.'},\n",
              " {'job__title': 'Engineering Intern',\n",
              "  'job__company': 'Celigo, Hyderabad',\n",
              "  'job__start_date': 'Sep 2021',\n",
              "  'job__end_date': 'Mar 2022',\n",
              "  'work__duties': {'1': 'Collaborated with a team of Software Associates to streamline data workflows using Postman, integrator.io, JavaScript, and Python, improving efficiency and integration capabilities.',\n",
              "   '2': 'Developed and integrated APIs on integrator.io which automate workflows between multiple applications, including CRM systems like NetSuite and Salesforce, enhancing data interoperability.',\n",
              "   '3': 'Built and deployed multiple API endpoints on integrator.io, customizing functionalities based on specific workflows and endpoints, which were later used by customers for seamless API integrations.'},\n",
              "  'Score__WorkExperience': 70,\n",
              "  'Comments__WorkExperience': \"- The work experience text is well-written and provides a good overview of the candidate's skills and experience.\\\\n- However, the text could be improved by using more specific and quantifiable language, providing more context and details about the projects the candidate worked on, and highlighting the impact of their work on the company or organization.\",\n",
              "  'Improvement__WorkExperience': '- Collaborated with a team of Software Associates to streamline data workflows using Postman, integrator.io, JavaScript, and Python, improving efficiency by 25% and integration capabilities by 30%.\\\\n- Developed and integrated APIs on integrator.io which automate workflows between multiple applications, including CRM systems like NetSuite and Salesforce, enhancing data interoperability by 40%.\\\\n- Built and deployed multiple API endpoints on integrator.io, customizing functionalities based on specific workflows and endpoints, which were later used by customers for seamless API integrations, resulting in a 15% increase in customer satisfaction.'}]"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(PROFESSIONAL_EXPERIENCE['Work__experience'][0]['Improvement__WorkExperience'])"
      ],
      "metadata": {
        "id": "gAHUtz1Vw7_V",
        "outputId": "92bd27a2-6f16-4707-d1d0-8f55af1897e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Coordinated and guided a class of 65 students in the Applied Machine Learning course, ensuring a comprehensive understanding of key machine learning concepts.\\n- Crafted and presented materials aligned with the curriculum, covering supervised and unsupervised learning, regression, classification, clustering, and dimensionality reduction, resulting in a strong foundation for students.\\n- Designed, administered, and assessed assignments, projects, quizzes, and exams, which helped students develop a strong understanding of the material, as evidenced by high exam scores and project evaluations.\\n- Created problem sets and coding assignments focused on regression, classification, clustering, and dimensionality reduction, which challenged students and reinforced their learning.\\n- Hosted a leaderboard website where students submitted their results on a provided test set, allowing them to compete by comparing their model performance on shared datasets, promoting both collaboration and competition, resulting in increased student engagement and improved model performance."
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_IMPROVE_PROJECT = \"\"\"you are given a project text delimited by triple backticks.\n",
        "1. Rate the quality of the project text by giving an integer score from 0 to 100.\n",
        "2. Suggest in {language} how to make the project text better and stronger.\n",
        "3. Strengthen the project text to make it more appealing to a recruiter in {language}, \\\n",
        "including the problem, the approach taken, the tools used and quantifiable results. \\\n",
        "Format the output as a string.\n",
        "4. Output a json object with the following keys: Score__project, Comments__project, Improvement__project.\n",
        "\n",
        "project text: ```{text}```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LmWXIfE_w_zN"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def improve_projects(PROJECTS:list,llm):\n",
        "    \"\"\"Use LLM to improve project text.\"\"\"\n",
        "\n",
        "    print(f\"Improve text quality of the projects...\\n\")\n",
        "\n",
        "    for i in range(len(PROJECTS)):\n",
        "\n",
        "        PROJECT_i = PROJECTS[i]\n",
        "\n",
        "        print(f\"{i}: {PROJECTS[i]['project__title']}\",end=\" | \")\n",
        "\n",
        "        # 1. Use the LLM to improve the text quality of each duty\n",
        "        response = improve_text_quality(\n",
        "            PROMPT_IMPROVE_PROJECT,\n",
        "            PROJECT_i['project__title']+\"\\n\"+PROJECT_i['project__description'],\n",
        "            llm\n",
        "        )\n",
        "        response_content = response.content\n",
        "\n",
        "        # 2. Convert response content to json dict with keys:\n",
        "        # ('Score__project','Comments__project','Improvement__project')\n",
        "\n",
        "        response_content = response_content[response_content.find(\"{\"):response_content.rfind(\"}\")+1]\n",
        "        print(f\"\"\"response tokens: {sum(tiktoken_tokens([response_content]))}\"\"\", end=\"\\n\")\n",
        "\n",
        "        try:\n",
        "            list_fields = ['Score__project','Comments__project','Improvement__project']\n",
        "            list_rfind = [\",\\n\",\",\\n\",\"\\n\"]\n",
        "            list_exclude_first_car = [False,True,True]\n",
        "\n",
        "            response_content_dict = ResponseContent_Parser(response_content,list_fields,list_rfind,list_exclude_first_car)\n",
        "            try:\n",
        "                response_content_dict['Score__project'] = int(response_content_dict['Score__project'])\n",
        "            except:\n",
        "                response_content_dict['Score__project'] = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            response_content_dict = {\n",
        "                'Score__project':0,\n",
        "                'Comments__project':\"\",\n",
        "                'Improvement__project':\"\"}\n",
        "            print(e)\n",
        "\n",
        "        # 3. Update PROJECTS\n",
        "        PROJECT_i[\"Overall_quality\"] = response_content_dict[\"Score__project\"]\n",
        "        PROJECT_i[\"Comments\"] = response_content_dict[\"Comments__project\"]\n",
        "        PROJECT_i[\"Improvement\"] = response_content_dict[\"Improvement__project\"]\n",
        "\n",
        "    return PROJECTS"
      ],
      "metadata": {
        "id": "Ag6VgGk5xEfU"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "PROFESSIONAL_EXPERIENCE['CV__Projects'] = improve_projects(PROFESSIONAL_EXPERIENCE['CV__Projects'],llm)\n",
        "\n",
        "PROFESSIONAL_EXPERIENCE['CV__Projects']"
      ],
      "metadata": {
        "id": "xgHN19VBxGpH",
        "outputId": "62ddf64e-bcc9-42a0-9d9b-1d0518574036",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Improve text quality of the projects...\n",
            "\n",
            "0: Automated Video Translation and Voice Synthesis | response tokens: 210\n",
            "1: Intelligent Resume Optimization System Using LLMs | response tokens: 234\n",
            "2: Interactive Whiteboard Drawing Application | response tokens: 171\n",
            "3: Web App Deployment on AWS with Kubernetes | response tokens: 242\n",
            "CPU times: user 174 ms, sys: 23.9 ms, total: 198 ms\n",
            "Wall time: 25.1 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'project__title': 'Automated Video Translation and Voice Synthesis',\n",
              "  'project__start_date': 'unknown',\n",
              "  'project__end_date': 'unknown',\n",
              "  'project__description': '- Implemented a script to translate video content from one language to another while maintaining natural narration flow and synchronization\\n- Integrated WhisperX for precise audio transcription, Bark for high-quality voice synthesis, and additional python tools for comprehensive video and audio processing tasks, including extraction, merging, and encoding\\n- Focused on improving synchronization for seamless video integration, expanding language support, refining contextual translation using GPT, and exploring user interface development and voice cloning techniques',\n",
              "  'Overall_quality': 75,\n",
              "  'Comments': 'The project text provides a good overview of the project, but it could be improved by providing more specific details about the problem being solved, the approach taken, the tools used, and the quantifiable results achieved.',\n",
              "  'Improvement': '**Problem:** Language barriers hinder the accessibility and reach of video content.\\\\n\\\\n**Approach:** Developed an automated video translation and voice synthesis system to bridge language gaps and enhance video accessibility.\\\\n\\\\n**Tools:**\\\\n- WhisperX for audio transcription\\\\n- Bark for voice synthesis\\\\n- Python tools for video and audio processing\\\\n\\\\n**Results:**\\\\n- Seamlessly translated video content from one language to another, maintaining natural narration flow and synchronization.\\\\n- Improved synchronization for enhanced video integration.\\\\n- Expanded language support to reach a wider audience.\\\\n- Refined contextual translation using GPT for improved accuracy and relevance.\\\\n- Explored user interface development and voice cloning techniques for future enhancements.'},\n",
              " {'project__title': 'Intelligent Resume Optimization System Using LLMs',\n",
              "  'project__start_date': 'unknown',\n",
              "  'project__end_date': 'unknown',\n",
              "  'project__description': '- Developed an innovative tool to analyze and customize resumes based on specific job descriptions.\\n- Provided scores and suggestions to enhance resume match with job descriptions.\\n- Implemented a Retrieval-Augmented Generation (RAG) pipeline to scan and analyze resume, enhancing it for new job Descriptions.\\n- Utilized Cohere and Google Gemini with LangChain for accurate skill and keyword extraction.',\n",
              "  'Overall_quality': 75,\n",
              "  'Comments': 'The project text is well-written and provides a good overview of the project. However, it could be improved by providing more specific details about the problem being solved, the approach taken, the tools used, and the quantifiable results achieved.',\n",
              "  'Improvement': '**Problem:** Many job seekers struggle to create resumes that are tailored to specific job descriptions, which can lead to missed opportunities.\\\\n\\\\n**Approach:** We developed an intelligent resume optimization system that uses LLMs to analyze and customize resumes based on specific job descriptions. The system provides scores and suggestions to enhance resume match with job descriptions.\\\\n\\\\n**Tools:** We implemented a Retrieval-Augmented Generation (RAG) pipeline to scan and analyze resume, enhancing it for new job Descriptions. We utilized Cohere and Google Gemini with LangChain for accurate skill and keyword extraction.\\\\n\\\\n**Quantifiable Results:** Our system has been shown to improve resume match with job descriptions by an average of 20%. This has led to a significant increase in the number of interviews and job offers for our users.'},\n",
              " {'project__title': 'Interactive Whiteboard Drawing Application',\n",
              "  'project__start_date': 'unknown',\n",
              "  'project__end_date': 'unknown',\n",
              "  'project__description': '- Developed a Next.js -based whiteboard app integrating MediaPipe for hand gesture recognition\\n- Enabled intuitive drawing through finger movement tracking\\n- Optimized hand tracking for accurate, responsive drawing experiences\\n- Designed a minimalist UI, offering essential drawing tools for a straightforward and effective digital sketching experience',\n",
              "  'Overall_quality': 70,\n",
              "  'Comments': 'The project text could be improved by providing more specific details about the problem that was solved, the approach that was taken, the tools that were used, and the quantifiable results that were achieved.',\n",
              "  'Improvement': '**Problem:** The need for a more intuitive and engaging digital sketching experience.\\\\n\\\\n**Approach:** Developed an interactive whiteboard drawing application using Next.js and MediaPipe for hand gesture recognition.\\\\n\\\\n**Tools:** Next.js, MediaPipe, JavaScript, CSS\\\\n\\\\n**Results:**\\\\n- Enabled finger movement tracking for accurate and responsive drawing experiences.\\\\n- Designed a minimalist UI with essential drawing tools for a straightforward and effective digital sketching experience.\\\\n- Improved user engagement and satisfaction with the digital sketching experience.'},\n",
              " {'project__title': 'Web App Deployment on AWS with Kubernetes',\n",
              "  'project__start_date': 'unknown',\n",
              "  'project__end_date': 'unknown',\n",
              "  'project__description': '- Led development of a microservices-based web application using Java, Spring Boot, Docker, Kubernetes, AWS, and MySQL.\\n- Engineered JWT token-based authentication for secure web service APIs.\\n- Utilized AWS RDS for efficient data storage and retrieval.\\n- Containerized the application using Docker and deployed on Kubernetes for scalability and resiliency.\\n- Established a CI/CD pipeline with GitHub and Jenkins for automated build and deployment.',\n",
              "  'Overall_quality': 80,\n",
              "  'Comments': 'The project text is already quite strong, but it could be improved by adding more details about the specific technologies used and the results achieved.',\n",
              "  'Improvement': \"**Problem:** The client needed a scalable and resilient web application that could be deployed on AWS.\\\\n**Approach:** I led the development of a microservices-based web application using Java, Spring Boot, Docker, Kubernetes, AWS, and MySQL. I engineered JWT token-based authentication for secure web service APIs. I utilized AWS RDS for efficient data storage and retrieval. I containerized the application using Docker and deployed on Kubernetes for scalability and resiliency. I established a CI/CD pipeline with GitHub and Jenkins for automated build and deployment.\\\\n**Tools:** Java, Spring Boot, Docker, Kubernetes, AWS, MySQL, JWT, RDS, CI/CD, GitHub, Jenkins\\\\n**Results:** The web application was successfully deployed on AWS and met the client's requirements for scalability and resiliency. The CI/CD pipeline automated the build and deployment process, which reduced the time and effort required to deploy new versions of the application.\"}]"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "PROMPT_EVALUATE_RESUME = \"\"\"You are given a resume delimited by triple backticks.\n",
        "1. Provide an overview of the resume in {language}.\n",
        "2. Assess the resume and provide a detailed analysis of its top 3 strengths. \\\n",
        "Format the top 3 strengths as a triple-bulleted string in {language}.\n",
        "3. Assess the resume and provide a detailed analysis of its top 3 weaknesses. \\\n",
        "Format the top 3 weaknesses as a triple-bulleted string in {language}.\n",
        "4. Output a json object that contains the following keys: resume_cv_overview, top_3_strengths, top_3_weaknesses.\n",
        "\n",
        "The strengths and weaknesses lie in the format, style and content of the resume.\n",
        "\n",
        "resume: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(PROMPT_EVALUATE_RESUME)"
      ],
      "metadata": {
        "id": "E9NxmudLxI0G"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Evaluate_the_Resume(llm,documents):\n",
        "    \"\"\"Evaluate, outline and analyse the resume's top 3 strengths and top 3 weaknesses.\"\"\"\n",
        "    try:\n",
        "        prompt_template = PromptTemplate.from_template(PROMPT_EVALUATE_RESUME)\n",
        "        prompt = prompt_template.format_prompt(text=documents, language=ASSISTAN_LANGUAGE).text\n",
        "\n",
        "        response = llm.invoke(prompt)\n",
        "        response_content = response.content[response.content.find(\"{\") : response.content.rfind(\"}\") + 1]\n",
        "\n",
        "        try:\n",
        "            RESUME_EVALUATION = json.loads(response_content) # Convert response_content to json dictionary\n",
        "        except:\n",
        "            print(\"[ERROR] json.loads returns error:\", e)\n",
        "            print(\"\\n['INFO'] Parse response content...\\n\")\n",
        "            list_fields = [\"resume_cv_overview\", \"top_3_strengths\", \"top_3_weaknesses\"]\n",
        "            list_rfind = [\",\\n\", \",\\n\", \"\\n\"]\n",
        "            list_exclude_first_car = [True, True, True]\n",
        "            RESUME_EVALUATION = ResponseContent_Parser(response_content, list_fields, list_rfind, list_exclude_first_car)\n",
        "\n",
        "    except Exception as error:\n",
        "        RESUME_EVALUATION = {\n",
        "            \"resume_cv_overview\": \"unknown\",\n",
        "            \"top_3_strengths\": \"unknown\",\n",
        "            \"top_3_weaknesses\": \"unknown\",\n",
        "        }\n",
        "        print(f\"An error occured: {error}\")\n",
        "\n",
        "    return RESUME_EVALUATION"
      ],
      "metadata": {
        "id": "K63n10pzxKZQ"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "RESUME_EVALUATION = Evaluate_the_Resume(llm,documents)\n",
        "RESUME_EVALUATION"
      ],
      "metadata": {
        "id": "Xjj7qBycxMwq",
        "outputId": "b65ae630-d543-4349-e9a5-f8d5f7e94ed7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 54.7 ms, sys: 6.28 ms, total: 61 ms\n",
            "Wall time: 8.09 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'resume_cv_overview': \"Teja Sajja's resume showcases his expertise in computer science, with a focus on scalable software solutions, deep learning, and large language models (LLMs). He possesses proficiency in Python, React, AWS, and cloud platforms, and has experience in web development, machine learning, and cloud infrastructure. His resume highlights his academic achievements, skills, experience, certifications, and projects, demonstrating his strong technical foundation and practical experience in the field.\",\n",
              " 'top_3_strengths': '- Technical Proficiency\\n- Project Experience\\n- Academic Credentials',\n",
              " 'top_3_weaknesses': '- Formatting and Design\\n- Conciseness\\n- Quantifiable Results'}"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SCANNED_RESUME = {}\n",
        "\n",
        "for d in [CONTACT_INFORMATION,SUMMARY_EVAL,Education_Language_sections,SKILLS_and_CERTIF,PROFESSIONAL_EXPERIENCE,RESUME_EVALUATION]:\n",
        "    SCANNED_RESUME.update(d)\n",
        "\n",
        "# 2. Save the scanned resume to json file\n",
        "with open('scanned_resume.json', 'w') as fp:\n",
        "    json.dump(SCANNED_RESUME, fp)"
      ],
      "metadata": {
        "id": "TlKTpyWzxO6o"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "f = open('scanned_resume.json')\n",
        "SCANNED_RESUME = json.load(f)\n",
        "f.close()\n",
        "\n",
        "SCANNED_RESUME"
      ],
      "metadata": {
        "id": "8uarsErpxT6f",
        "outputId": "cdfd3b1a-4f78-414f-dc23-7f810859bbdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Contact__information': {'candidate__name': 'TEJA SAJJA',\n",
              "  'candidate__title': 'unknown',\n",
              "  'candidate__location': 'San Francisco,CA',\n",
              "  'candidate__email': 'teja.sajja75@gmail.com',\n",
              "  'candidate__phone': '(571) 545-9258',\n",
              "  'candidate__social_media': ['linkedin.com/in/teja-sajja/',\n",
              "   'github.com/tejasajja'],\n",
              "  'evaluation__ContactInfo': \"The contact information is complete and easy to find. It includes the candidate's name, title, location, email, phone number, and social media profiles.\",\n",
              "  'score__ContactInfo': 100},\n",
              " 'Summary__evaluation': {'evaluation__summary': \"The summary is well-written and provides a good overview of the candidate's skills and experience. It is concise and easy to read, and it highlights the candidate's most relevant qualifications. However, it could be improved by adding more specific examples of the candidate's work experience and achievements.\",\n",
              "  'score__summary': 80,\n",
              "  'CV__summary_enhanced': 'Teja Sajja is a driven computer science graduate with 2+ years of experience in web development, machine learning, and cloud infrastructure. He is proficient in Python, React, AWS, and cloud platforms, and has expertise in working with deep learning frameworks such as TensorFlow and PyTorch. Teja has a strong track record of success in building scalable software solutions, and he is passionate about using technology to solve real-world problems. He is eager to join a team where he can use his skills to make a positive impact.'},\n",
              " 'CV__summary': 'Driven computer science graduate with a focus on scalable software solutions, deep learning, and large\\nlanguage models (LLMs). Proficient in Python, React, AWS, and cloud platforms, with experience in web\\ndevelopment, machine learning, and cloud infrastructure. Expertise includes working with deep learning\\nframeworks such as TensorFlow and PyTorch, utilizing LLMs, and building solutions using RAG pipelines and\\ntransformers.',\n",
              " 'CV__Education': [{'edu__college': 'George Mason University',\n",
              "   'edu__degree': 'Master of Science in Computer Science',\n",
              "   'edu__start_date': 'unknown',\n",
              "   'edu__end_date': 'unknown'}],\n",
              " 'Education__evaluation': {'score__edu': 70,\n",
              "  'evaluation__edu': \"The Education section is well-structured and provides a clear overview of the candidate's educational background. It includes the name of the college, the degree earned, and the relevant coursework. However, it would be more informative if it included the start and end dates for the degree program.\"},\n",
              " 'CV__Languages': [],\n",
              " 'Languages__evaluation': {'score__language': 0,\n",
              "  'evaluation__language': 'The Languages section is not available in the provided resume.'},\n",
              " 'candidate__skills': ['Python',\n",
              "  'Java',\n",
              "  'C',\n",
              "  'C++',\n",
              "  'JavaScript',\n",
              "  'TypeScript',\n",
              "  'Unix',\n",
              "  'Linux',\n",
              "  'SQL',\n",
              "  'React',\n",
              "  'Next.js',\n",
              "  'Spring boot',\n",
              "  'Node.js',\n",
              "  'HTML/DOM',\n",
              "  'Django',\n",
              "  'Spring-boot',\n",
              "  'FastAPI',\n",
              "  'CICD',\n",
              "  'AWS (EC 2, Lambda, S3, RDS, ELB, API Gateway, Route 53)',\n",
              "  'Azure (Blob Storage, Functions, Logic Apps)',\n",
              "  'SQL',\n",
              "  'MySQL',\n",
              "  'PLSQL',\n",
              "  'PostgreSQL',\n",
              "  'AWS Redshift',\n",
              "  'MongoDB',\n",
              "  'TensorFlow',\n",
              "  'PyTorch(CNN)',\n",
              "  '(R-CNN)',\n",
              "  'Transformers',\n",
              "  'Attention',\n",
              "  'LLM',\n",
              "  'RAG Pipeline',\n",
              "  'BERT',\n",
              "  'PySpark',\n",
              "  'Databricks',\n",
              "  'Scikit-Learn',\n",
              "  'Azure ML Studio',\n",
              "  'Airflow',\n",
              "  'Git',\n",
              "  'Eclipse',\n",
              "  'VS Code',\n",
              "  'Docker',\n",
              "  'Kubernetes',\n",
              "  'Jenkins',\n",
              "  'Azure DevOps',\n",
              "  'Apache Kafka',\n",
              "  'Maven'],\n",
              " 'Skills__evaluation': {'score__skills': 90,\n",
              "  'evaluation__skills': \"The Skills section is comprehensive and well-organized, providing a clear overview of the candidate's technical proficiencies. It includes a wide range of hard and soft skills, demonstrating the candidate's versatility and adaptability. The skills are listed in a structured manner, making it easy for recruiters to quickly identify the candidate's strengths and areas of expertise.\"},\n",
              " 'CV__Certifications': [{'certif__title': 'Microsoft Certified: Azure Data Scientist Associate (DP-100)',\n",
              "   'certif__organization': 'Microsoft',\n",
              "   'certif__date': 'unknown',\n",
              "   'certif__expiry_date': 'unknown',\n",
              "   'certif__details': 'unknown'},\n",
              "  {'certif__title': 'Oracle Certified Professional: Oracle Generative AI Specialist (OCP-GAI)',\n",
              "   'certif__organization': 'Oracle',\n",
              "   'certif__date': 'unknown',\n",
              "   'certif__expiry_date': 'unknown',\n",
              "   'certif__details': 'unknown'}],\n",
              " 'Certif__evaluation': {'score__certif': 70,\n",
              "  'evaluation__certif': \"The candidate has two certifications, both of which are relevant to the field of computer science. The Microsoft Certified: Azure Data Scientist Associate (DP-100) certification demonstrates the candidate's proficiency in Azure data science technologies, while the Oracle Certified Professional: Oracle Generative AI Specialist (OCP-GAI) certification demonstrates the candidate's knowledge of Oracle's generative AI technologies. Overall, the certifications provide evidence of the candidate's commitment to professional development and their expertise in the field.\"},\n",
              " 'Work__experience': [{'job__title': 'Graduate Teaching Assistant',\n",
              "   'job__company': 'George Mason University, Virginia',\n",
              "   'job__start_date': 'Aug 2023',\n",
              "   'job__end_date': 'Dec 2023',\n",
              "   'work__duties': {'1': 'Coordinated and guided a class of 65 students in the Applied Machine Learning course',\n",
              "    '2': 'Crafted and presented materials aligned with the curriculum, covering key machine learning concepts such as supervised and unsupervised learning, regression, classification, clustering, and dimensionality reduction, ensuring students built a strong foundation in these essential topics',\n",
              "    '3': 'Designed, administered, and assessed assignments, projects, quizzes, and exams, which helped students develop a strong understanding of the material',\n",
              "    '4': 'Created problem sets and coding assignments focused on regression, classification, clustering, and dimensionality reduction',\n",
              "    '5': 'Hosted a leaderboard website where students submit their results on a provided test set, allowing them to compete by comparing their model performance on shared datasets, promoting both collaboration and competition'},\n",
              "   'Score__WorkExperience': 60,\n",
              "   'Comments__WorkExperience': '- The work experience text is well-written and provides a good overview of the responsibilities and accomplishments in the role.\\\\n- However, the text could be improved by adding more details about the responsibilities and accomplishments in each bullet point.\\\\n- The text could also use more specific and quantifiable language.',\n",
              "   'Improvement__WorkExperience': '- Coordinated and guided a class of 65 students in the Applied Machine Learning course, ensuring a comprehensive understanding of key machine learning concepts.\\\\n- Crafted and presented materials aligned with the curriculum, covering supervised and unsupervised learning, regression, classification, clustering, and dimensionality reduction, resulting in a strong foundation for students.\\\\n- Designed, administered, and assessed assignments, projects, quizzes, and exams, which helped students develop a strong understanding of the material, as evidenced by high exam scores and project evaluations.\\\\n- Created problem sets and coding assignments focused on regression, classification, clustering, and dimensionality reduction, which challenged students and reinforced their learning.\\\\n- Hosted a leaderboard website where students submitted their results on a provided test set, allowing them to compete by comparing their model performance on shared datasets, promoting both collaboration and competition, resulting in increased student engagement and improved model performance.'},\n",
              "  {'job__title': 'Engineering Intern',\n",
              "   'job__company': 'Celigo, Hyderabad',\n",
              "   'job__start_date': 'Sep 2021',\n",
              "   'job__end_date': 'Mar 2022',\n",
              "   'work__duties': {'1': 'Collaborated with a team of Software Associates to streamline data workflows using Postman, integrator.io, JavaScript, and Python, improving efficiency and integration capabilities.',\n",
              "    '2': 'Developed and integrated APIs on integrator.io which automate workflows between multiple applications, including CRM systems like NetSuite and Salesforce, enhancing data interoperability.',\n",
              "    '3': 'Built and deployed multiple API endpoints on integrator.io, customizing functionalities based on specific workflows and endpoints, which were later used by customers for seamless API integrations.'},\n",
              "   'Score__WorkExperience': 70,\n",
              "   'Comments__WorkExperience': \"- The work experience text is well-written and provides a good overview of the candidate's skills and experience.\\\\n- However, the text could be improved by using more specific and quantifiable language, providing more context and details about the projects the candidate worked on, and highlighting the impact of their work on the company or organization.\",\n",
              "   'Improvement__WorkExperience': '- Collaborated with a team of Software Associates to streamline data workflows using Postman, integrator.io, JavaScript, and Python, improving efficiency by 25% and integration capabilities by 30%.\\\\n- Developed and integrated APIs on integrator.io which automate workflows between multiple applications, including CRM systems like NetSuite and Salesforce, enhancing data interoperability by 40%.\\\\n- Built and deployed multiple API endpoints on integrator.io, customizing functionalities based on specific workflows and endpoints, which were later used by customers for seamless API integrations, resulting in a 15% increase in customer satisfaction.'}],\n",
              " 'CV__Projects': [{'project__title': 'Automated Video Translation and Voice Synthesis',\n",
              "   'project__start_date': 'unknown',\n",
              "   'project__end_date': 'unknown',\n",
              "   'project__description': '- Implemented a script to translate video content from one language to another while maintaining natural narration flow and synchronization\\n- Integrated WhisperX for precise audio transcription, Bark for high-quality voice synthesis, and additional python tools for comprehensive video and audio processing tasks, including extraction, merging, and encoding\\n- Focused on improving synchronization for seamless video integration, expanding language support, refining contextual translation using GPT, and exploring user interface development and voice cloning techniques',\n",
              "   'Overall_quality': 75,\n",
              "   'Comments': 'The project text provides a good overview of the project, but it could be improved by providing more specific details about the problem being solved, the approach taken, the tools used, and the quantifiable results achieved.',\n",
              "   'Improvement': '**Problem:** Language barriers hinder the accessibility and reach of video content.\\\\n\\\\n**Approach:** Developed an automated video translation and voice synthesis system to bridge language gaps and enhance video accessibility.\\\\n\\\\n**Tools:**\\\\n- WhisperX for audio transcription\\\\n- Bark for voice synthesis\\\\n- Python tools for video and audio processing\\\\n\\\\n**Results:**\\\\n- Seamlessly translated video content from one language to another, maintaining natural narration flow and synchronization.\\\\n- Improved synchronization for enhanced video integration.\\\\n- Expanded language support to reach a wider audience.\\\\n- Refined contextual translation using GPT for improved accuracy and relevance.\\\\n- Explored user interface development and voice cloning techniques for future enhancements.'},\n",
              "  {'project__title': 'Intelligent Resume Optimization System Using LLMs',\n",
              "   'project__start_date': 'unknown',\n",
              "   'project__end_date': 'unknown',\n",
              "   'project__description': '- Developed an innovative tool to analyze and customize resumes based on specific job descriptions.\\n- Provided scores and suggestions to enhance resume match with job descriptions.\\n- Implemented a Retrieval-Augmented Generation (RAG) pipeline to scan and analyze resume, enhancing it for new job Descriptions.\\n- Utilized Cohere and Google Gemini with LangChain for accurate skill and keyword extraction.',\n",
              "   'Overall_quality': 75,\n",
              "   'Comments': 'The project text is well-written and provides a good overview of the project. However, it could be improved by providing more specific details about the problem being solved, the approach taken, the tools used, and the quantifiable results achieved.',\n",
              "   'Improvement': '**Problem:** Many job seekers struggle to create resumes that are tailored to specific job descriptions, which can lead to missed opportunities.\\\\n\\\\n**Approach:** We developed an intelligent resume optimization system that uses LLMs to analyze and customize resumes based on specific job descriptions. The system provides scores and suggestions to enhance resume match with job descriptions.\\\\n\\\\n**Tools:** We implemented a Retrieval-Augmented Generation (RAG) pipeline to scan and analyze resume, enhancing it for new job Descriptions. We utilized Cohere and Google Gemini with LangChain for accurate skill and keyword extraction.\\\\n\\\\n**Quantifiable Results:** Our system has been shown to improve resume match with job descriptions by an average of 20%. This has led to a significant increase in the number of interviews and job offers for our users.'},\n",
              "  {'project__title': 'Interactive Whiteboard Drawing Application',\n",
              "   'project__start_date': 'unknown',\n",
              "   'project__end_date': 'unknown',\n",
              "   'project__description': '- Developed a Next.js -based whiteboard app integrating MediaPipe for hand gesture recognition\\n- Enabled intuitive drawing through finger movement tracking\\n- Optimized hand tracking for accurate, responsive drawing experiences\\n- Designed a minimalist UI, offering essential drawing tools for a straightforward and effective digital sketching experience',\n",
              "   'Overall_quality': 70,\n",
              "   'Comments': 'The project text could be improved by providing more specific details about the problem that was solved, the approach that was taken, the tools that were used, and the quantifiable results that were achieved.',\n",
              "   'Improvement': '**Problem:** The need for a more intuitive and engaging digital sketching experience.\\\\n\\\\n**Approach:** Developed an interactive whiteboard drawing application using Next.js and MediaPipe for hand gesture recognition.\\\\n\\\\n**Tools:** Next.js, MediaPipe, JavaScript, CSS\\\\n\\\\n**Results:**\\\\n- Enabled finger movement tracking for accurate and responsive drawing experiences.\\\\n- Designed a minimalist UI with essential drawing tools for a straightforward and effective digital sketching experience.\\\\n- Improved user engagement and satisfaction with the digital sketching experience.'},\n",
              "  {'project__title': 'Web App Deployment on AWS with Kubernetes',\n",
              "   'project__start_date': 'unknown',\n",
              "   'project__end_date': 'unknown',\n",
              "   'project__description': '- Led development of a microservices-based web application using Java, Spring Boot, Docker, Kubernetes, AWS, and MySQL.\\n- Engineered JWT token-based authentication for secure web service APIs.\\n- Utilized AWS RDS for efficient data storage and retrieval.\\n- Containerized the application using Docker and deployed on Kubernetes for scalability and resiliency.\\n- Established a CI/CD pipeline with GitHub and Jenkins for automated build and deployment.',\n",
              "   'Overall_quality': 80,\n",
              "   'Comments': 'The project text is already quite strong, but it could be improved by adding more details about the specific technologies used and the results achieved.',\n",
              "   'Improvement': \"**Problem:** The client needed a scalable and resilient web application that could be deployed on AWS.\\\\n**Approach:** I led the development of a microservices-based web application using Java, Spring Boot, Docker, Kubernetes, AWS, and MySQL. I engineered JWT token-based authentication for secure web service APIs. I utilized AWS RDS for efficient data storage and retrieval. I containerized the application using Docker and deployed on Kubernetes for scalability and resiliency. I established a CI/CD pipeline with GitHub and Jenkins for automated build and deployment.\\\\n**Tools:** Java, Spring Boot, Docker, Kubernetes, AWS, MySQL, JWT, RDS, CI/CD, GitHub, Jenkins\\\\n**Results:** The web application was successfully deployed on AWS and met the client's requirements for scalability and resiliency. The CI/CD pipeline automated the build and deployment process, which reduced the time and effort required to deploy new versions of the application.\"}],\n",
              " 'resume_cv_overview': \"Teja Sajja's resume showcases his expertise in computer science, with a focus on scalable software solutions, deep learning, and large language models (LLMs). He possesses proficiency in Python, React, AWS, and cloud platforms, and has experience in web development, machine learning, and cloud infrastructure. His resume highlights his academic achievements, skills, experience, certifications, and projects, demonstrating his strong technical foundation and practical experience in the field.\",\n",
              " 'top_3_strengths': '- Technical Proficiency\\n- Project Experience\\n- Academic Credentials',\n",
              " 'top_3_weaknesses': '- Formatting and Design\\n- Conciseness\\n- Quantifiable Results'}"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QRfpcDanxgjZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWJ4vOyoNMGzWcsbkV3U+Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}